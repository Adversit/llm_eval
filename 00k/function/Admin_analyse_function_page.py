import streamlit as st
import pandas as pd
import os
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
from collections import Counter
import json
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from function.llm_service import load_llm_config, get_llm_analysis, get_llm_analysis_stream

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

def get_available_projects():
    """è·å–dataç›®å½•ä¸‹æ‰€æœ‰å¯ç”¨çš„è¯„ä¼°é¡¹ç›®"""
    data_dir = "data"
    projects = []
    
    if not os.path.exists(data_dir):
        return projects
    
    for item in os.listdir(data_dir):
        item_path = os.path.join(data_dir, item)
        if os.path.isdir(item_path):
            # æŸ¥æ‰¾è¯„ä¼°ç»“æœæ–‡ä»¶
            result_files = [f for f in os.listdir(item_path) if f.endswith('_è¯„ä¼°ç»“æœ.xlsx')]
            if result_files:
                # å°è¯•è¯»å–é¡¹ç›®ä¿¡æ¯
                json_files = [f for f in os.listdir(item_path) if f.endswith('.json') and not f.endswith('_è¯„ä¼°ç»“æœ.json')]
                project_info = None
                if json_files:
                    try:
                        with open(os.path.join(item_path, json_files[0]), 'r', encoding='utf-8') as f:
                            project_info = json.load(f)
                    except:
                        project_info = None
                
                projects.append({
                    'folder_name': item,
                    'result_file': os.path.join(item_path, result_files[0]),
                    'project_info': project_info,
                    'display_name': item.replace('_', ' - ')
                })
    
    return projects

def load_questionnaire_results(file_path):
    """åŠ è½½é—®å·ç»“æœæ•°æ®"""
    try:
        df = pd.read_excel(file_path)
        return df
    except Exception as e:
        st.error(f"åŠ è½½é—®å·ç»“æœå¤±è´¥: {e}")
        return None

def load_questionnaire_file(project_folder):
    """åŠ è½½åŸå§‹é—®å·æ–‡ä»¶ä»¥è·å–å®Œæ•´é€‰é¡¹æ–‡æœ¬"""
    try:
        questionnaire_files = [f for f in os.listdir(project_folder) if f.endswith('_é—®å·.xlsx')]
        if questionnaire_files:
            questionnaire_path = os.path.join(project_folder, questionnaire_files[0])
            df = pd.read_excel(questionnaire_path)
            return df
        return None
    except Exception as e:
        return None

def get_option_mapping(questionnaire_df, question_text):
    """ä»åŸå§‹é—®å·è·å–é€‰é¡¹å­—æ¯åˆ°å®Œæ•´æ–‡æœ¬çš„æ˜ å°„"""
    try:
        question_row = questionnaire_df[questionnaire_df['é—®é¢˜ä¸»å¹²'] == question_text]
        if not question_row.empty:
            options_text = question_row.iloc[0]['ç­”æ¡ˆé€‰é¡¹']
            if pd.notna(options_text):
                options_list = options_text.split('|')
                # åˆ›å»ºå­—æ¯åˆ°é€‰é¡¹çš„æ˜ å°„ A->é€‰é¡¹1, B->é€‰é¡¹2 ç­‰
                mapping = {}
                for i, option in enumerate(options_list):
                    letter = chr(65 + i)  # A=65, B=66, C=67...
                    mapping[letter] = option.strip()
                return mapping
    except:
        pass
    return {}

def analyze_single_choice_question(df, question_num, question_text, questionnaire_df=None):
    """åˆ†æå•é€‰é¢˜"""
    question_data = df[df['é¢˜å·'] == question_num]
    
    if question_data.empty:
        return None

    total_responses = len(question_data)
    if total_responses == 0:
        return None
    
    # è®¾å®šé€‰é¡¹æ–‡æœ¬é•¿åº¦é˜ˆå€¼
    TEXT_LIMIT = 20
    
    # è·å–é€‰é¡¹æ˜ å°„
    option_mapping = get_option_mapping(questionnaire_df, question_text) if questionnaire_df is not None else {}
    
    # å¦‚æœæœ‰é€‰é¡¹æ˜ å°„ï¼Œæ˜¾ç¤ºæ‰€æœ‰é€‰é¡¹ï¼›å¦åˆ™åªæ˜¾ç¤ºæœ‰ç­”æ¡ˆçš„é€‰é¡¹
    if option_mapping:
        # æ˜¾ç¤ºæ‰€æœ‰é€‰é¡¹ï¼ŒåŒ…æ‹¬æœªé€‰æ‹©çš„
        all_options = sorted(option_mapping.keys())
        display_labels = []
        display_values = []
        
        for option_letter in all_options:
            count = question_data[question_data['å›ç­”'] == option_letter].shape[0]
            proportion = count / total_responses
            full_option = option_mapping[option_letter]
            # æˆªæ–­è¿‡é•¿çš„é€‰é¡¹æ–‡æœ¬
            if len(full_option) > TEXT_LIMIT:
                full_option = full_option[:TEXT_LIMIT] + "..."
            display_labels.append(f"{option_letter}. {full_option}")
            display_values.append(proportion)
    else:
        # æ²¡æœ‰é€‰é¡¹æ˜ å°„æ—¶ï¼Œç»Ÿè®¡å®é™…ç­”æ¡ˆ
        answer_counts = question_data['å›ç­”'].value_counts().sort_index()
        display_labels = []
        display_values = []
        for option_letter, count in answer_counts.items():
            proportion = count / total_responses
            display_labels.append(option_letter)
            display_values.append(proportion)
    
    # å¤„ç†æ ‡é¢˜ï¼Œé¿å…é‡å¤é¢˜å·
    title = question_text if str(question_num) + 'ã€' in question_text else f"{question_num}ã€{question_text}"
    
    # åˆ›å»ºplotlyæŸ±çŠ¶å›¾
    fig = go.Figure(data=go.Bar(
        x=display_labels,
        y=display_values,
        texttemplate='%{y:.1%}',
        textposition='outside',
        marker_color='lightblue'
    ))
    
    fig.update_layout(
        title=dict(text=title, font=dict(size=14)),
        xaxis_title="",
        yaxis_title="é€‰æ‹©æ¯”ä¾‹",
        showlegend=False,
        height=500,  # å›ºå®šé«˜åº¦
        width=None,  # è®©å®½åº¦è‡ªé€‚åº”
        xaxis_tickangle=-45,
        margin=dict(l=50, r=50, t=80, b=150),  # å›ºå®šè¾¹è·
        xaxis=dict(tickfont=dict(size=10)),
        yaxis=dict(tickfont=dict(size=10), tickformat=".0%", range=[0, 1.05])
    )
    
    return fig

def analyze_multiple_choice_question(df, question_num, question_text, questionnaire_df=None):
    """åˆ†æå¤šé€‰é¢˜"""
    question_data = df[df['é¢˜å·'] == question_num]
    
    if question_data.empty:
        return None

    total_responses = len(question_data)
    if total_responses == 0:
        return None
    
    # è®¾å®šé€‰é¡¹æ–‡æœ¬é•¿åº¦é˜ˆå€¼
    TEXT_LIMIT = 20
    
    # è·å–é€‰é¡¹æ˜ å°„
    option_mapping = get_option_mapping(questionnaire_df, question_text) if questionnaire_df is not None else {}
    
    # ç»Ÿè®¡æ¯ä¸ªé€‰é¡¹è¢«é€‰æ‹©çš„æ¬¡æ•°
    option_counts = Counter()
    
    for answer in question_data['å›ç­”']:
        if pd.notna(answer) and answer:
            # åˆ†å‰²å¤šé€‰ç­”æ¡ˆï¼ˆç”¨åˆ†å·åˆ†éš”ï¼‰
            options = answer.split(';')
            for option in options:
                option = option.strip()
                if option:
                    option_counts[option] += 1
    
    # å¦‚æœæœ‰é€‰é¡¹æ˜ å°„ï¼Œæ˜¾ç¤ºæ‰€æœ‰é€‰é¡¹ï¼›å¦åˆ™åªæ˜¾ç¤ºæœ‰é€‰æ‹©çš„é€‰é¡¹
    if option_mapping:
        # æ˜¾ç¤ºæ‰€æœ‰é€‰é¡¹ï¼ŒåŒ…æ‹¬æœªé€‰æ‹©çš„
        all_options = sorted(option_mapping.keys())
        option_data = []
        
        for option_letter in all_options:
            count = option_counts.get(option_letter, 0)  # æœªé€‰æ‹©çš„é€‰é¡¹è®¡æ•°ä¸º0
            proportion = count / total_responses
            full_option = option_mapping[option_letter]
            # æˆªæ–­è¿‡é•¿çš„é€‰é¡¹æ–‡æœ¬
            if len(full_option) > TEXT_LIMIT:
                full_option = full_option[:TEXT_LIMIT] + "..."
            display_label = f"{option_letter}. {full_option}"
            option_data.append({
                'letter': option_letter,
                'display_label': display_label,
                'count': proportion
            })
    else:
        # æ²¡æœ‰é€‰é¡¹æ˜ å°„æ—¶ï¼Œåªæ˜¾ç¤ºæœ‰é€‰æ‹©çš„é€‰é¡¹
        option_data = []
        for option_letter, count in option_counts.items():
            proportion = count / total_responses
            option_data.append({
                'letter': option_letter,
                'display_label': option_letter,
                'count': proportion
            })
        # æŒ‰å­—æ¯æ’åº
        option_data.sort(key=lambda x: x['letter'])
    
    display_labels = [item['display_label'] for item in option_data]
    display_values = [item['count'] for item in option_data]
    
    # å¤„ç†æ ‡é¢˜ï¼Œé¿å…é‡å¤é¢˜å·
    title = question_text if str(question_num) + 'ã€' in question_text else f"{question_num}ã€{question_text}"
    
    # åˆ›å»ºplotlyæŸ±çŠ¶å›¾
    fig = go.Figure(data=go.Bar(
        x=display_labels,
        y=display_values,
        texttemplate='%{y:.1%}',
        textposition='outside',
        marker_color='lightcoral'
    ))
    
    fig.update_layout(
        title=dict(text=title, font=dict(size=14)),
        xaxis_title="",
        yaxis_title="é€‰æ‹©æ¯”ä¾‹",
        showlegend=False,
        height=500,  # å›ºå®šé«˜åº¦ï¼Œä¸å•é€‰é¢˜å®Œå…¨ä¸€è‡´
        width=None,  # è®©å®½åº¦è‡ªé€‚åº”
        xaxis_tickangle=-45,
        margin=dict(l=50, r=50, t=80, b=150),  # å›ºå®šè¾¹è·ï¼Œä¸å•é€‰é¢˜å®Œå…¨ä¸€è‡´
        xaxis=dict(tickfont=dict(size=10)),
        yaxis=dict(tickfont=dict(size=10), tickformat=".0%")
    )
    
    return fig

def generate_overall_statistics(df):
    """ç”Ÿæˆæ•´ä½“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        total_responses = len(df['é—®å·ID'].unique()) if 'é—®å·ID' in df.columns else len(df)
    except:
        total_responses = 0
    
    total_questions = len(df['é¢˜å·'].unique())
    single_choice_count = len(df[df['é—®é¢˜ç±»å‹'] == 'å•é€‰é¢˜']['é¢˜å·'].unique())
    multiple_choice_count = len(df[df['é—®é¢˜ç±»å‹'] == 'å¤šé€‰é¢˜']['é¢˜å·'].unique())
    
    return {
        'total_responses': total_responses,
        'total_questions': total_questions,
        'single_choice_count': single_choice_count,
        'multiple_choice_count': multiple_choice_count
    }

def create_capability_distribution_chart(df):
    """åˆ›å»ºèƒ½åŠ›é¡¹åˆ†å¸ƒå›¾"""
    item_counts = df.groupby('èƒ½åŠ›é¡¹')['é¢˜å·'].nunique().reset_index()
    item_counts.columns = ['èƒ½åŠ›é¡¹', 'é¢˜ç›®æ•°é‡']
    item_counts = item_counts.sort_values('é¢˜ç›®æ•°é‡', ascending=True)
    
    fig = go.Figure(data=go.Bar(
        x=item_counts['é¢˜ç›®æ•°é‡'],
        y=item_counts['èƒ½åŠ›é¡¹'],
        orientation='h',
        marker_color='lightgreen'
    ))
    
    fig.update_layout(
        title="å„èƒ½åŠ›é¡¹é¢˜ç›®æ•°é‡åˆ†å¸ƒ",
        xaxis_title="é¢˜ç›®æ•°é‡",
        yaxis_title="èƒ½åŠ›é¡¹",
        height=max(400, len(item_counts) * 30)
    )
    
    return fig

def calculate_question_expectation(df, question_num):
    """è®¡ç®—å•ä¸ªé—®é¢˜çš„æœŸæœ›å€¼"""
    question_data = df[df['é¢˜å·'] == question_num]
    if question_data.empty:
        return None

    total_responses = len(question_data)
    if total_responses == 0:
        return None

    score_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5}
    
    answer_counts = question_data['å›ç­”'].value_counts()
    
    total_score = 0
    for answer, count in answer_counts.items():
        total_score += score_map.get(str(answer).strip(), 0) * count
        
    expected_value = total_score / total_responses
    return expected_value

def analyze_user_demand_matching(df):
    """åˆ†æç”¨æˆ·éœ€æ±‚åŒ¹é…åº¦"""
    keywords = {
        'understanding': "æ˜¯å¦èƒ½å‡†ç¡®ç†è§£æ‚¨çš„åŸºæœ¬æŸ¥è¯¢éœ€æ±‚",
        'accuracy': "æä¾›çš„ä¿¡æ¯å‡†ç¡®ç‡å¤§çº¦åœ¨ä»€ä¹ˆæ°´å¹³",
        'satisfaction': "å›åº”æ‚¨éœ€æ±‚çš„æ»¡æ„åº¦æ˜¯"
    }
    
    results = {}
    unique_questions = df[['é¢˜å·', 'é—®é¢˜']].drop_duplicates()
    question_info = {}

    for q_type, keyword in keywords.items():
        matched_row = unique_questions[unique_questions['é—®é¢˜'].str.contains(keyword, na=False)]
        if not matched_row.empty:
            q_num = matched_row.iloc[0]['é¢˜å·']
            q_text = matched_row.iloc[0]['é—®é¢˜']
            
            expectation = calculate_question_expectation(df, q_num)
            if expectation is not None:
                question_info[q_type] = {
                    'text': q_text,
                    'num': q_num,
                    'expectation': expectation,
                    'score': int(expectation)
                }

    if len(question_info) == 3:
        final_score = min(info['score'] for info in question_info.values())
        results['final_score'] = final_score
        results['details'] = question_info
        
        rating_descriptions = {
            1: "å¤§æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«åŸºæœ¬çš„ç”¨æˆ·éœ€æ±‚ï¼Œä½†å‡†ç¡®æ€§å’Œç›¸å…³æ€§è¾ƒä½",
            2: "å¤§æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å’Œç†è§£è¾ƒä¸ºå¤æ‚çš„ç”¨æˆ·éœ€æ±‚ï¼Œæä¾›çš„ä¿¡æ¯æœ‰ä¸€å®šçš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§",
            3: "å¤§æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å’Œç†è§£å¤æ‚çš„ç”¨æˆ·éœ€æ±‚ï¼Œæä¾›çš„ä¿¡æ¯é«˜åº¦ç›¸å…³ä¸”å‡†ç¡®",
            4: "å¤§æ¨¡å‹ä¸ä»…èƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å’Œç†è§£å¤æ‚çš„ç”¨æˆ·éœ€æ±‚ï¼Œè¿˜èƒ½æä¾›ä¸ªæ€§åŒ–çš„å»ºè®®å’Œè§£å†³æ–¹æ¡ˆ",
            5: "å¤§æ¨¡å‹åœ¨è¯†åˆ«å’Œç†è§£ç”¨æˆ·éœ€æ±‚æ–¹é¢è¾¾åˆ°å“è¶Šæ°´å¹³ï¼Œèƒ½å¤Ÿå®æ—¶é€‚åº”ç”¨æˆ·éœ€æ±‚å˜åŒ–å¹¶æä¾›æœ€ä¼˜è§£å†³æ–¹æ¡ˆ"
        }
        results['final_rating_description'] = rating_descriptions.get(final_score, "æ— è¯„çº§")

        return results
    else:
        return None

def analyze_automation_improvement(df):
    """åˆ†æä¸šåŠ¡è‡ªåŠ¨åŒ–æå‡ç‡"""
    keywords = {
        'complexity': "èƒ½å¤Ÿè‡ªåŠ¨åŒ–å¤„ç†çš„ä¸šåŠ¡å¤æ‚åº¦å¦‚ä½•",
        'intervention': "å®Œæˆä¸šåŠ¡æµç¨‹æ—¶ï¼Œéœ€è¦äººå·¥å¹²é¢„å’Œç›‘ç£çš„ä»»åŠ¡å æ¯”çº¦ä¸º",
        'improvement': "æ‚¨ä¼°è®¡ä¸šåŠ¡è‡ªåŠ¨åŒ–ç¨‹åº¦æå‡äº†å¤šå°‘ï¼Ÿ"
    }
    
    results = {}
    unique_questions = df[['é¢˜å·', 'é—®é¢˜']].drop_duplicates()
    question_info = {}

    for q_type, keyword in keywords.items():
        matched_row = unique_questions[unique_questions['é—®é¢˜'].str.contains(keyword, na=False)]
        if not matched_row.empty:
            q_num = matched_row.iloc[0]['é¢˜å·']
            q_text = matched_row.iloc[0]['é—®é¢˜']
            
            expectation = calculate_question_expectation(df, q_num)
            if expectation is not None:
                question_info[q_type] = {
                    'text': q_text,
                    'num': q_num,
                    'expectation': expectation,
                    'score': int(expectation)
                }

    if len(question_info) == 3:
        final_score = min(info['score'] for info in question_info.values())
        results['final_score'] = final_score
        results['details'] = question_info
        
        rating_descriptions = {
            1: "ä»…èƒ½å®ç°éƒ¨åˆ†ä»»åŠ¡çš„è‡ªåŠ¨åŒ–ï¼Œéœ€è¦å¤§é‡äººå·¥å¹²é¢„",
            2: "èƒ½å¤Ÿå®ç°è¾ƒå¤šä»»åŠ¡çš„è‡ªåŠ¨åŒ–ï¼Œä½†ä»éœ€ç›¸å½“ä¸€éƒ¨åˆ†äººå·¥ç›‘ç£",
            3: "èƒ½å¤Ÿå®ç°å¤§éƒ¨åˆ†ä»»åŠ¡çš„è‡ªåŠ¨åŒ–ï¼Œäººå·¥å¹²é¢„è¾ƒå°‘",
            4: "èƒ½å¤Ÿå®ç°å‡ ä¹æ‰€æœ‰ä»»åŠ¡çš„è‡ªåŠ¨åŒ–ï¼Œäººå·¥å¹²é¢„æå°‘",
            5: "èƒ½å¤Ÿå®ç°å®Œå…¨çš„ä»»åŠ¡è‡ªåŠ¨åŒ–ï¼Œå‡ ä¹æ— éœ€äººå·¥å¹²é¢„"
        }
        results['final_rating_description'] = rating_descriptions.get(final_score, "æ— è¯„çº§")

        return results
    else:
        return None

def analyze_decision_support(df):
    """åˆ†æä¸šåŠ¡å†³ç­–æ”¯æŒåŠ›"""
    keywords = {
        'effectiveness': "è¾…åŠ©ä¸šåŠ¡å·¥ä½œå†³ç­–çš„æ•ˆæœå¦‚ä½•",
        'coverage': "å¯æ”¯æŒçš„å†³ç­–ç¯èŠ‚å æ•´ä¸ªä¸šåŠ¡å†³ç­–é“¾æ¡",
        'accuracy': "åœ¨ç†è§£æ‚¨çš„è¦æ±‚æ—¶ï¼Œå‡†ç¡®ç‡çº¦ä¸ºå¤šå°‘"
    }
    
    results = {}
    unique_questions = df[['é¢˜å·', 'é—®é¢˜']].drop_duplicates()
    question_info = {}

    for q_type, keyword in keywords.items():
        matched_row = unique_questions[unique_questions['é—®é¢˜'].str.contains(keyword, na=False)]
        if not matched_row.empty:
            q_num = matched_row.iloc[0]['é¢˜å·']
            q_text = matched_row.iloc[0]['é—®é¢˜']
            
            expectation = calculate_question_expectation(df, q_num)
            if expectation is not None:
                question_info[q_type] = {
                    'text': q_text,
                    'num': q_num,
                    'expectation': expectation,
                    'score': int(expectation)
                }

    if len(question_info) == 3:
        final_score = min(info['score'] for info in question_info.values())
        results['final_score'] = final_score
        results['details'] = question_info
        
        rating_descriptions = {
            1: "ç»„ç»‡å°šæœªè®¤è¯†åˆ°å¤§æ¨¡å‹åœ¨ä¸šåŠ¡å†³ç­–ä¸­çš„æ½œåŠ›ï¼Œæˆ–å°šæœªå¼€å§‹ç›¸å…³æŠ€æœ¯çš„æ¢ç´¢ä¸åº”ç”¨",
            2: "ç»„ç»‡å¼€å§‹å°è¯•å°†å¤§æ¨¡å‹æŠ€æœ¯åº”ç”¨äºä¸šåŠ¡å†³ç­–æ”¯æŒï¼Œè¿›è¡Œåˆæ­¥çš„ä¸šåŠ¡é¢„æµ‹ä¸åˆ†æ",
            3: "ç»„ç»‡èƒ½å¤Ÿæœ‰æ•ˆç®¡ç†å’Œæ•´åˆå¤§æ¨¡å‹æŠ€æœ¯ï¼Œç”¨äºæ”¯æŒå¤æ‚å†³ç­–ï¼Œå¹¶å®ç°ä¸€å®šç¨‹åº¦çš„è‡ªåŠ¨åŒ–",
            4: "ç»„ç»‡èƒ½å¤Ÿåˆ©ç”¨å¤§æ¨¡å‹æŠ€æœ¯å®ç°é«˜åº¦æ™ºèƒ½åŒ–çš„ä¸šåŠ¡å†³ç­–æ”¯æŒï¼Œæ˜¾è‘—ä¼˜åŒ–å†³ç­–æ•ˆç‡ä¸æ•ˆæœ",
            5: "ç»„ç»‡ä¸ä»…èƒ½å¤Ÿé«˜æ•ˆåˆ©ç”¨å¤§æ¨¡å‹ä¼˜åŒ–å½“å‰å†³ç­–ï¼Œè¿˜èƒ½é¢„è§æœªæ¥è¶‹åŠ¿ï¼Œå®ç°çœŸæ­£çš„è‡ªä¸»ä¸ååŒå†³ç­–"
        }
        results['final_rating_description'] = rating_descriptions.get(final_score, "æ— è¯„çº§")

        return results
    else:
        return None

def analyze_customer_loyalty(df):
    """åˆ†æå®¢æˆ·å¿ è¯šåº¦"""
    keywords = {
        'trust': "æ¥å—ä¸ä¿¡ä»»ç¨‹åº¦å¦‚ä½•",
        'frequency': "é«˜é¢‘ä½¿ç”¨"
    }
    
    results = {}
    unique_questions = df[['é¢˜å·', 'é—®é¢˜']].drop_duplicates()
    question_info = {}

    for q_type, keyword in keywords.items():
        matched_row = unique_questions[unique_questions['é—®é¢˜'].str.contains(keyword, na=False)]
        if not matched_row.empty:
            q_num = matched_row.iloc[0]['é¢˜å·']
            q_text = matched_row.iloc[0]['é—®é¢˜']
            
            expectation = calculate_question_expectation(df, q_num)
            if expectation is not None:
                question_info[q_type] = {
                    'text': q_text,
                    'num': q_num,
                    'expectation': expectation,
                    'score': int(expectation)
                }

    if len(question_info) == 2:
        final_score = min(info['score'] for info in question_info.values())
        results['final_score'] = final_score
        results['details'] = question_info
        
        rating_descriptions = {
            1: "å®¢æˆ·å¿ è¯šåº¦æå‡æœ‰é™ï¼Œç”¨æˆ·æ¨èæ„æ„¿è¾ƒä½ï¼Œç•™å­˜ç‡è¾ƒå·®",
            2: "å®¢æˆ·å¿ è¯šåº¦æœ‰æ‰€æå‡ï¼Œç”¨æˆ·ä½“éªŒæœ‰å¾…æ”¹è¿›",
            3: "å®¢æˆ·å¿ è¯šåº¦æ˜¾è‘—æå‡ï¼Œåº”ç”¨è¾ƒä¸ºæˆç†Ÿï¼Œç”¨æˆ·ç•™å­˜ç‡è¾ƒå¥½",
            4: "å®¢æˆ·å¿ è¯šåº¦æå¤§æå‡ï¼Œåº”ç”¨éå¸¸æˆç†Ÿï¼Œèƒ½å¤Ÿæä¾›é«˜åº¦ä¸ªæ€§åŒ–å’Œç²¾å‡†çš„æœåŠ¡",
            5: "å®¢æˆ·å¿ è¯šåº¦è¾¾åˆ°æè‡´ï¼Œç”¨æˆ·ä½“éªŒæä¸ºå‡ºè‰²ï¼Œç”¨æˆ·å¯¹åº”ç”¨çš„ä¾èµ–æ€§å’Œæ»¡æ„åº¦æé«˜"
        }
        results['final_rating_description'] = rating_descriptions.get(final_score, "æ— è¯„çº§")

        return results
    else:
        return None

def analyze_time_cost_saving(df):
    """åˆ†ææ—¶é—´æˆæœ¬èŠ‚ç´„ç‡"""
    keyword = "æ˜¯å¦èƒ½å¤Ÿå¸®åŠ©æ‚¨æœ‰æ•ˆé™ä½å·¥ä½œæ—¶é—´æˆæœ¬"
    q_type = "saving"
    
    results = {}
    unique_questions = df[['é¢˜å·', 'é—®é¢˜']].drop_duplicates()
    question_info = {}
    
    matched_row = unique_questions[unique_questions['é—®é¢˜'].str.contains(keyword, na=False)]
    if not matched_row.empty:
        q_num = matched_row.iloc[0]['é¢˜å·']
        q_text = matched_row.iloc[0]['é—®é¢˜']
        
        expectation = calculate_question_expectation(df, q_num)
        if expectation is not None:
            score = int(expectation)
            question_info[q_type] = {
                'text': q_text,
                'num': q_num,
                'expectation': expectation,
                'score': score
            }

    if len(question_info) == 1:
        final_score = question_info[q_type]['score']
        results['final_score'] = final_score
        results['details'] = question_info
        
        rating_descriptions = {
            1: "å¤§æ¨¡å‹åœ¨é‡‘èä¸šåŠ¡åœºæ™¯ä¸­å¯¹æ—¶é—´æˆæœ¬çš„èŠ‚çº¦æ•ˆæœè¾ƒä½ï¼Œä»»åŠ¡å®Œæˆæ—¶é—´å‡å°‘ä¸æ˜æ˜¾",
            2: "å¤§æ¨¡å‹åœ¨é‡‘èä¸šåŠ¡åœºæ™¯ä¸­å¯¹æ—¶é—´æˆæœ¬çš„èŠ‚çº¦æœ‰æ‰€æå‡ï¼Œä½†ä»æœ‰è¾ƒå¤§çš„ä¼˜åŒ–ç©ºé—´",
            3: "å¤§æ¨¡å‹åœ¨é‡‘èä¸šåŠ¡åœºæ™¯ä¸­å¯¹æ—¶é—´æˆæœ¬çš„èŠ‚çº¦è¾¾åˆ°ä¸­ç­‰æ°´å¹³ï¼Œä»»åŠ¡å®Œæˆæ—¶é—´æ˜¾è‘—å‡å°‘",
            4: "å¤§æ¨¡å‹åœ¨é‡‘èä¸šåŠ¡åœºæ™¯ä¸­å¯¹æ—¶é—´æˆæœ¬çš„èŠ‚çº¦æ•ˆæœè¾ƒå¥½ï¼Œä»»åŠ¡å®Œæˆæ—¶é—´å¤§å¹…å‡å°‘",
            5: "å¤§æ¨¡å‹åœ¨é‡‘èä¸šåŠ¡åœºæ™¯ä¸­å¯¹æ—¶é—´æˆæœ¬çš„èŠ‚çº¦æ•ˆæœæä½³ï¼Œä»»åŠ¡å®Œæˆæ—¶é—´æ˜¾è‘—ç¼©çŸ­"
        }
        results['final_rating_description'] = rating_descriptions.get(final_score, "æ— è¯„çº§")
        return results
            
    return None

def display_analysis_results(title, results, metric_labels):
    """Helper function to display analysis results in a consistent format."""
    st.markdown("---")
    st.markdown(f'<h3 style="text-align: center;">{title}è¯„çº§</h3>', unsafe_allow_html=True)
    if results:
        details = results['details']
        final_score = results['final_score']
        final_desc = results['final_rating_description']
        
        metric_cols = st.columns(len(metric_labels))
        for i, (q_type, label) in enumerate(metric_labels.items()):
            with metric_cols[i]:
                score = details[q_type]['score']
                help_text = f"åŸºäºé—®é¢˜: \"{details[q_type]['text']}\"\næœŸæœ›å€¼ä¸º: {details[q_type]['expectation']:.2f}"
                
                # ä½¿ç”¨Markdownå’ŒHTMLå®ç°å±…ä¸­å¯¹é½çš„æŒ‡æ ‡å¡
                st.markdown(f"""
                <div style="text-align: center;">
                    <p style="font-size: 1.11rem; margin-bottom: 0;">
                        {label}å¾—åˆ†
                    </p>
                    <p style="font-size: 2rem; margin-top: 0;">{score}</p>
                </div>
                """, unsafe_allow_html=True)

        score_to_level = {1: "ä¸€çº§", 2: "äºŒçº§", 3: "ä¸‰çº§", 4: "å››çº§", 5: "äº”çº§"}
        level_text = score_to_level.get(final_score, "")
        
        fig_gauge = go.Figure(go.Indicator(
            mode="gauge",
            value=final_score,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': ""},
            gauge={
                'axis': {'range': [0, 5], 'tickwidth': 1, 'tickcolor': "darkblue"},
                'bar': {'color': "#C00000", 'thickness': 0.4},
                'steps': [
                    {'range': [0, 1], 'color': "#E3F2FD"},
                    {'range': [1, 2], 'color': "#90CAF9"},
                    {'range': [2, 3], 'color': "#42A5F5"},
                    {'range': [3, 4], 'color': "#1E88E5"},
                    {'range': [4, 5], 'color': "#0D47A1"},
                ],
                'borderwidth': 0,
                 'threshold': {
                    'line': {'color': "#8E0000", 'width': 4},
                    'thickness': 0.75,
                    'value': final_score
                }
            }
        ))


        fig_gauge.add_annotation(
            x=0.5,
            y=0.19,
            text=f"{level_text}",
            showarrow=False,
            font=dict(
                family="Microsoft YaHei",
                size=48,
                color="#000000"
            )
        )

        fig_gauge.update_layout(height=400, margin=dict(l=20, r=20, t=0, b=20))
        st.plotly_chart(fig_gauge, use_container_width=True)
        
        st.markdown(
            f'<div style="background-color: #F2F2F2; padding: 12px 10px; border-radius: 5px; margin-bottom: 40px;">'
            f'<b>è¯„çº§è¯´æ˜ï¼š</b> {final_desc}'
            '</div>',
            unsafe_allow_html=True
        )
    else:
        st.info(f"â„¹ï¸ é—®å·ä¸­æœªæ‰¾åˆ°ç”¨äºè®¡ç®—**{title}**çš„å…¨éƒ¨ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚")

def Admin_analyse_function_page():
    """æ•°æ®åˆ†æé¡µé¢ä¸»å‡½æ•°"""
    st.title("ğŸ“Š é—®å·ç»“æœæ•°æ®åˆ†æ")
    st.markdown("### è¯„ä¼°æ•°æ®å¯è§†åŒ–åˆ†æå·¥å…·")
    st.markdown("---")
    
    # è·å–å¯ç”¨é¡¹ç›®
    projects = get_available_projects()
    
    if not projects:
        st.warning("ğŸ” æœªæ‰¾åˆ°ä»»ä½•è¯„ä¼°ç»“æœæ•°æ®")
        st.info("""
        **è¯´æ˜ï¼š**
        - è¯·ç¡®ä¿dataç›®å½•ä¸‹å­˜åœ¨è¯„ä¼°é¡¹ç›®æ–‡ä»¶å¤¹
        - æ¯ä¸ªé¡¹ç›®æ–‡ä»¶å¤¹ä¸­åº”åŒ…å«"[é¡¹ç›®å]_è¯„ä¼°ç»“æœ.xlsx"æ–‡ä»¶
        - è¯„ä¼°ç»“æœæ–‡ä»¶ç”±é—®å·é‡‡é›†é¡µé¢è‡ªåŠ¨ç”Ÿæˆ
        """)
        return
    
    # é¡¹ç›®é€‰æ‹©
    st.header(" é¡¹ç›®é€‰æ‹©")
    
    project_options = {proj['display_name']: proj for proj in projects}
    selected_project_name = st.selectbox(
        "é€‰æ‹©è¦åˆ†æçš„è¯„ä¼°é¡¹ç›®",
        options=list(project_options.keys()),
        help="é€‰æ‹©ä¸€ä¸ªå·²å®Œæˆè¯„ä¼°çš„é¡¹ç›®è¿›è¡Œæ•°æ®åˆ†æ"
    )
    
    if not selected_project_name:
        return
    
    selected_project = project_options[selected_project_name]
    
    # æ˜¾ç¤ºé¡¹ç›®ä¿¡æ¯
    st.markdown("---")
    if selected_project['project_info']:
        eval_info = selected_project['project_info'].get('evaluation_info', {})
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("å…¬å¸åç§°", eval_info.get('company_name', 'æœªçŸ¥'))
        with col2:
            st.metric("åœºæ™¯åç§°", eval_info.get('scenario_name', 'æœªçŸ¥'))
        with col3:
            st.metric("åˆ›å»ºæ—¶é—´", eval_info.get('created_time', 'æœªçŸ¥')[:10] if eval_info.get('created_time') else 'æœªçŸ¥')
        with col4:
            st.metric("è¯„ä¼°çŠ¶æ€", eval_info.get('status', 'æœªçŸ¥'))

    df = load_questionnaire_results(selected_project['result_file'])
    if df is None:
        return
    
    # åŠ è½½åŸå§‹é—®å·æ–‡ä»¶ä»¥è·å–å®Œæ•´é€‰é¡¹æ–‡æœ¬
    project_folder = os.path.dirname(selected_project['result_file'])
    questionnaire_df = load_questionnaire_file(project_folder)
    
    # æ•´ä½“ç»Ÿè®¡
    stats = generate_overall_statistics(df)

    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("é—®å·ä»½æ•°", stats['total_responses'])
    with col2:
        st.metric("é¢˜ç›®æ€»æ•°", stats['total_questions'])
    with col3:
        st.metric("å•é€‰é¢˜æ•°", stats['single_choice_count'])
    with col4:
        st.metric("å¤šé€‰é¢˜æ•°", stats['multiple_choice_count'])
    
    # èƒ½åŠ›é¡¹åˆ†å¸ƒå›¾
    capability_fig = create_capability_distribution_chart(df)
    st.plotly_chart(capability_fig, use_container_width=True)
    
    # é¢˜ç›®åˆ†æ - æŒ‰é¢˜ç›®é¡ºåºï¼Œæ¯æ’2ä¸ªå›¾
    
    # è·å–å”¯ä¸€çš„é¢˜ç›®ä¿¡æ¯
    questions_info = df[['é¢˜å·', 'é—®é¢˜', 'é—®é¢˜ç±»å‹']].drop_duplicates().sort_values('é¢˜å·')
    
    # æŒ‰é¢˜ç›®é¡ºåºæ˜¾ç¤ºï¼Œæ¯æ’2ä¸ªå›¾
    questions_list = list(questions_info.iterrows())
    for i in range(0, len(questions_list), 2):
        col1, col2 = st.columns(2)
        
        # ç¬¬ä¸€ä¸ªå›¾
        if i < len(questions_list):
            _, row1 = questions_list[i]
            question_num1 = row1['é¢˜å·']
            question_text1 = row1['é—®é¢˜']
            question_type1 = row1['é—®é¢˜ç±»å‹']
            
            with col1:
                if question_type1 == 'å•é€‰é¢˜':
                    fig1 = analyze_single_choice_question(df, question_num1, question_text1, questionnaire_df)
                    if fig1:
                        st.plotly_chart(fig1, use_container_width=True)
                elif question_type1 == 'å¤šé€‰é¢˜':
                    fig1 = analyze_multiple_choice_question(df, question_num1, question_text1, questionnaire_df)
                    if fig1:
                        st.plotly_chart(fig1, use_container_width=True)
        
        # ç¬¬äºŒä¸ªå›¾
        if i + 1 < len(questions_list):
            _, row2 = questions_list[i + 1]
            question_num2 = row2['é¢˜å·']
            question_text2 = row2['é—®é¢˜']
            question_type2 = row2['é—®é¢˜ç±»å‹']
            
            with col2:
                if question_type2 == 'å•é€‰é¢˜':
                    fig2 = analyze_single_choice_question(df, question_num2, question_text2, questionnaire_df)
                    if fig2:
                        st.plotly_chart(fig2, use_container_width=True)
                elif question_type2 == 'å¤šé€‰é¢˜':
                    fig2 = analyze_multiple_choice_question(df, question_num2, question_text2, questionnaire_df)
                    if fig2:
                        st.plotly_chart(fig2, use_container_width=True)

    # ç»¼åˆç»´åº¦åˆ†æ
    st.markdown("---")
    st.header("ğŸ“ˆ å„é¡¹è¯„çº§")

    analysis_tasks = [
        {
            "title": "ç”¨æˆ·éœ€æ±‚åŒ¹é…åº¦",
            "analyzer": analyze_user_demand_matching,
            "labels": {
                'understanding': "ç†è§£èƒ½åŠ›",
                'accuracy': "ä¿¡æ¯å‡†ç¡®ç‡",
                'satisfaction': "ç”¨æˆ·æ»¡æ„åº¦"
            }
        },
        {
            "title": "ä¸šåŠ¡è‡ªåŠ¨åŒ–æå‡ç‡",
            "analyzer": analyze_automation_improvement,
            "labels": {
                'complexity': "å¤„ç†å¤æ‚åº¦",
                'intervention': "äººå·¥å¹²é¢„åº¦",
                'improvement': "è‡ªåŠ¨åŒ–æå‡ç‡"
            }
        },
        {
            "title": "ä¸šåŠ¡å†³ç­–æ”¯æŒåŠ›",
            "analyzer": analyze_decision_support,
            "labels": {
                'effectiveness': "å†³ç­–æ•ˆæœ",
                'coverage': "å†³ç­–è¦†ç›–ç‡",
                'accuracy': "ç†è§£å‡†ç¡®ç‡"
            }
        },
        {
            "title": "å®¢æˆ·å¿ è¯šåº¦",
            "analyzer": analyze_customer_loyalty,
            "labels": {
                'trust': "ä¿¡ä»»ä¸æ¥å—åº¦",
                'frequency': "ä½¿ç”¨é¢‘ç‡"
            }
        },
        {
            "title": "æ—¶é—´æˆæœ¬èŠ‚çº¦ç‡",
            "analyzer": analyze_time_cost_saving,
            "labels": {
                'saving': "æ—¶é—´èŠ‚çº¦æ•ˆæœ"
            }
        }
    ]

    reports = []
    for task in analysis_tasks:
        results = task["analyzer"](df)
        if results:
            reports.append({
                "title": task["title"],
                "results": results,
                "labels": task["labels"]
            })

    for i in range(0, len(reports), 2):
        cols = st.columns(2)

        # ç¬¬ä¸€ä¸ªå›¾
        with cols[0]:
            report = reports[i]
            display_analysis_results(report["title"], report["results"], report["labels"])

        # ç¬¬äºŒä¸ªå›¾
        if i + 1 < len(reports):
            with cols[1]:
                report = reports[i+1]
                display_analysis_results(report["title"], report["results"], report["labels"])

    def prepare_multichoice_data_for_ai(df, questionnaire_df):
        """å‡†å¤‡å¤šé€‰é¢˜æ•°æ®ä»¥ä¾›AIåˆ†æ"""
        multichoice_questions = df[df['é—®é¢˜ç±»å‹'] == 'å¤šé€‰é¢˜']
        unique_questions = multichoice_questions[['é¢˜å·', 'é—®é¢˜']].drop_duplicates().sort_values('é¢˜å·')
        
        analysis_data = []
        
        for _, row in unique_questions.iterrows():
            q_num = row['é¢˜å·']
            q_text = row['é—®é¢˜']
            
            question_data = df[df['é¢˜å·'] == q_num]
            total_responses = len(question_data['é—®å·ID'].unique())
            if total_responses == 0:
                continue

            option_mapping = get_option_mapping(questionnaire_df, q_text)
            
            option_counts = Counter()
            for answer in question_data['å›ç­”']:
                if pd.notna(answer) and answer:
                    options = answer.split(';')
                    for option in options:
                        option = option.strip()
                        if option:
                            option_counts[option] += 1
            
            options_summary = []
            sorted_options = sorted(option_mapping.keys()) if option_mapping else sorted(option_counts.keys())

            for option_letter in sorted_options:
                count = option_counts.get(option_letter, 0)
                proportion = count / total_responses if total_responses > 0 else 0
                option_text = option_mapping.get(option_letter, "æœªçŸ¥é€‰é¡¹")
                options_summary.append(f"  - é€‰é¡¹ {option_letter} ({option_text}): é€‰æ‹©æ¯”ä¾‹ {proportion:.1%}")
            
            analysis_data.append(f"é—®é¢˜ {q_num}ï¼š{q_text}\n" + "\n".join(options_summary))
            
        return "\n\n".join(analysis_data)

    def build_ai_prompt(multichoice_data_summary):
        """æ„å»ºç”¨äºAIåˆ†æçš„æç¤º"""
        return f"""
è¯·æ ¹æ®ä»¥ä¸‹å¤šé€‰é¢˜çš„è°ƒç ”æ•°æ®ï¼Œæ’°å†™ä¸€æ®µæ·±å…¥çš„åˆ†ææŠ¥å‘Šã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. **è¯†åˆ«æ ¸å¿ƒæ¨¡å¼**ï¼šä¸è¦ä»…ä»…å¤è¿°æ•°æ®ã€‚è¦æ‰¾å‡ºæ•°æ®èƒŒåéšè—çš„æ¨¡å¼ã€å…³è”æˆ–çŸ›ç›¾ä¹‹å¤„ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·åœ¨ä¸€ä¸ªé—®é¢˜ä¸Šé€‰æ‹©Aï¼Œåœ¨å¦ä¸€ä¸ªé—®é¢˜ä¸Šé€‰æ‹©Bï¼Œè¿™èƒŒåå¯èƒ½åæ˜ äº†ä»€ä¹ˆå…±åŒçš„å¿ƒæ€æˆ–ç°çŠ¶ï¼Ÿ
2. **æç‚¼æ ¸å¿ƒç»“è®º**ï¼šåŸºäºä½ å‘ç°çš„æ¨¡å¼ï¼Œæå‡º2-3ä¸ªæ ¸å¿ƒçš„ã€æœ‰ä»·å€¼çš„ç»“è®ºã€‚
3. **ä¸“ä¸šåŒ–è¯­è¨€**ï¼šä½¿ç”¨ä¸“ä¸šã€å®¢è§‚ã€æ­£å¼çš„åˆ†æå¸ˆè¯­è¨€ï¼Œé¿å…å£è¯­åŒ–è¡¨è¾¾ã€‚
4. **ç»“æ„æ¸…æ™°**ï¼šåˆ†ææŠ¥å‘Šåº”è¯¥é€»è¾‘è¿è´¯ï¼Œå½¢æˆä¸€ä¸ªæœ‰æœºæ•´ä½“ï¼Œè€Œä¸æ˜¯ç®€å•çš„è¦ç‚¹ç½—åˆ—ã€‚
5. **åªæ€»ç»“å‘ç°çš„æƒ…å†µï¼Œä¸æä¾›å»ºè®®**
ä»¥ä¸‹æ˜¯éœ€è¦åˆ†æçš„æ•°æ®ï¼š
---
{multichoice_data_summary}
---

è¯·å¼€å§‹ä½ çš„åˆ†æï¼Œç›´æ¥è¾“å‡ºåˆ†ææŠ¥å‘Šå³å¯ï¼Œä¸éœ€è¦åŒ…å«"å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†æ"ç­‰å¤šä½™çš„è¯ã€‚
"""

    def generate_deterministic_report(reports, total_responses):
        """
        ç”ŸæˆæŠ¥å‘Šçš„å‰ä¸¤ä¸ªç¡®å®šæ€§éƒ¨åˆ†ã€‚
        """
        report_parts = []
        final_rating_map = {1: "ä¸€çº§", 2: "äºŒçº§", 3: "ä¸‰çº§", 4: "å››çº§", 5: "äº”çº§"}

        # --- æ®µè½ä¸€ï¼šæ€»ç»“æ€§æ¦‚è¿° ---
        final_rating_score = min(report['results']['final_score'] for report in reports)
        final_rating_text = final_rating_map.get(final_rating_score, "æœªè¯„çº§")
        evaluated_dimensions = [report['title'] for report in reports]
        report_parts.append(" ")
        report_parts.append("#### ä¸€ã€ç»¼åˆè¯„ä¼°æ€»ç»“")
        summary_text = (
            f"æœ¬æ¬¡è¯„ä¼°çš„æœ€ç»ˆç»¼åˆè¯„çº§ä¸º**{final_rating_text}**ã€‚"
            f"è¯„ä¼°å…±è¦†ç›–{len(evaluated_dimensions)}ä¸ªèƒ½åŠ›åŸŸï¼ŒåŒ…æ‹¬ï¼š{', '.join(evaluated_dimensions)}ã€‚"
            f"æœ¬æ¬¡åˆ†æç»“æœåŸºäº{total_responses}ä»½æœ‰æ•ˆé—®å·ã€‚"
        )
        report_parts.append(summary_text)

        # --- æ®µè½äºŒï¼šåˆ†ç»´åº¦è¯¦ç»†åˆ†æ ---
        report_parts.append("#### äºŒã€å„ç»´åº¦è¡¨ç°è¯¦è¿°")
        for report in reports:
            title = report['title']
            results = report['results']
            labels = report['labels']
            
            score = results['final_score']
            rating_text = final_rating_map.get(score, "æœªè¯„çº§")
            description = results['final_rating_description']
            
            sub_items = [{"name": label, "score": results['details'][q_type]['score']} for q_type, label in labels.items()]
            
            dimension_analysis_detail = ""
            if not sub_items:
                dimension_analysis_detail = ""
            elif len(sub_items) == 1:
                score = sub_items[0]['score']
                dimension_analysis_detail = f"å…¶è¡¨ç°ç”±**{sub_items[0]['name']}**è¿™ä¸€å­é¡¹å†³å®šï¼Œå¾—åˆ†ä¸º{score}åˆ†ã€‚"
            else:
                min_score = min(item['score'] for item in sub_items)
                max_score = max(item['score'] for item in sub_items)

                if min_score == max_score:
                    dimension_analysis_detail = f"è¯¥ç»´åº¦ä¸‹çš„å„å­é¡¹è¡¨ç°å‡è¡¡ï¼Œå¾—åˆ†å‡ä¸º{min_score}åˆ†ã€‚"
                else:
                    lowest_items = [item['name'] for item in sub_items if item['score'] == min_score]
                    highest_items = [item['name'] for item in sub_items if item['score'] == max_score]
                    dimension_analysis_detail = (
                        f"è¯¥èƒ½åŠ›é¡¹è¯„çº§ä¸»è¦å—é™äº{', '.join(lowest_items)}ï¼ˆå¾—åˆ†: {min_score}ï¼‰ï¼Œ"
                        f"è€Œåœ¨{', '.join(highest_items)}ï¼ˆå¾—åˆ†: {max_score}ï¼‰æ–¹é¢è¡¨ç°ç›¸å¯¹è¾ƒå¥½ã€‚"
                    )
            
            dimension_analysis = (
                f"{title}çš„è¯„çº§ä¸º**{rating_text}**ï¼Œæœ¬åœºæ™¯{description}ã€‚"
                f"{dimension_analysis_detail}"
            )
            report_parts.append(dimension_analysis)
        
        return "\n\n".join(report_parts)

    # åˆ†ææŠ¥å‘Šç”Ÿæˆ
    st.markdown("---")
    st.header("ğŸ“Š è¯„çº§åˆ†ææŠ¥å‘Š")
    
    if reports:
        if st.button("âœ¨ ç‚¹å‡»ç”Ÿæˆåˆ†ææŠ¥å‘Š", type="primary", use_container_width=True):
            
            report_box_template = (
                '<div style="background-color: #F8F9FA; border: 1px solid #E9ECEF; padding: 20px; border-radius: 10px;">'
                '{content}'
                '</div>'
            )
            
            # 1. ç«‹å³ç”Ÿæˆå¹¶å±•ç¤ºç¡®å®šæ€§å†…å®¹
            deterministic_content = generate_deterministic_report(reports, stats['total_responses'])
            placeholder = st.empty()
            placeholder.markdown(report_box_template.format(content=deterministic_content), unsafe_allow_html=True)
            
            # 2. å‡†å¤‡AIåˆ†ææ‰€éœ€æ•°æ®
            llm_config = load_llm_config()
            multichoice_data = prepare_multichoice_data_for_ai(df, questionnaire_df)
            
            # 3. æ‰§è¡Œå¹¶æµå¼å±•ç¤ºAIåˆ†æ
            if llm_config and multichoice_data:
                api_key, base_url, model_name = None, None, None
                api_key = llm_config.get("api_key")
                base_url = llm_config.get("base_url")
                model_name = llm_config.get("model_name")
                if not all([api_key, base_url, model_name]):
                    first_key = next(iter(llm_config), None)
                    if first_key and isinstance(llm_config[first_key], dict):
                        config_dict = llm_config[first_key]
                        api_key = config_dict.get("api_key")
                        base_url = config_dict.get("base_url")
                        model_name = config_dict.get("model")

                prompt = build_ai_prompt(multichoice_data)
                
                # ä¸ºç¬¬ä¸‰éƒ¨åˆ†å‡†å¤‡å†…å®¹
                ai_content_header = "\n\n#### ä¸‰ã€å¤šé€‰é¢˜æ·±åº¦æ´å¯Ÿ\n\n"
                full_content_so_far = deterministic_content + ai_content_header
                ai_analysis_text = ""

                # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
                placeholder.markdown(report_box_template.format(content=full_content_so_far + "â³ è°ƒç”¨AIè¿›è¡Œæ·±åº¦åˆ†æä¸­..."), unsafe_allow_html=True)
                
                # è·å–å¹¶æ˜¾ç¤ºæµå¼å“åº”
                stream = get_llm_analysis_stream(prompt, api_key, base_url, model_name)
                for chunk in stream:
                    ai_analysis_text += chunk
                    placeholder.markdown(report_box_template.format(content=full_content_so_far + ai_analysis_text + "â–Œ"), unsafe_allow_html=True)
                
                # ç§»é™¤å…‰æ ‡å¹¶æœ€ç»ˆæ˜¾ç¤º
                final_content = full_content_so_far + ai_analysis_text
                placeholder.markdown(report_box_template.format(content=final_content), unsafe_allow_html=True)
                st.success("âœ… åˆ†ææŠ¥å‘Šå·²å…¨éƒ¨ç”Ÿæˆï¼")

            elif not llm_config:
                error_msg = "\n\n#### ä¸‰ã€å¤šé€‰é¢˜æ·±åº¦æ´å¯Ÿ\n\næœªèƒ½åŠ è½½`model_config.json`é…ç½®æ–‡ä»¶ï¼ŒAIåˆ†ææ¨¡å—æ— æ³•å¯åŠ¨ã€‚"
                placeholder.markdown(report_box_template.format(content=deterministic_content + error_msg), unsafe_allow_html=True)
            elif not multichoice_data:
                error_msg = "\n\n#### ä¸‰ã€å¤šé€‰é¢˜æ·±åº¦æ´å¯Ÿ\n\næŠ¥å‘Šä¸­æœªå‘ç°å¤šé€‰é¢˜æ•°æ®ï¼Œæ— æ³•è¿›è¡ŒAIæ·±åº¦æ´å¯Ÿã€‚"
                placeholder.markdown(report_box_template.format(content=deterministic_content + error_msg), unsafe_allow_html=True)

    else:
        st.info("â„¹ï¸ å½“å‰æ²¡æœ‰è¶³å¤Ÿçš„è¯„çº§æ•°æ®å¯ä¾›åˆ†æã€‚")

    # ç®€åŒ–çš„å¯¼å‡ºåŠŸèƒ½
    st.markdown("---")

if __name__ == "__main__":
    Admin_analyse_function_page() 