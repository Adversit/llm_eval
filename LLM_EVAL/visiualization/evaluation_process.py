import streamlit as st
import sys
from pathlib import Path
import time
from typing import Dict, Any, List, Optional
import pandas as pd
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from first_stage.stage1_evaluator import Stage1Evaluator
from second_stage.stage2_evaluator import Stage2Evaluator
from utils.result_processor import ResultProcessor
from utils.visual import DataVisualizer
from utils.file_manager_singleton import reset_file_manager_for_new_test

class ProgressTracker:
    """è¿›åº¦è¿½è¸ªå™¨ï¼Œæä¾›å‡†ç¡®çš„è¿›åº¦è®¡ç®—å’Œé¢„è®¡æ—¶é—´"""

    def __init__(self, total_items: int):
        """åˆå§‹åŒ–è¿›åº¦è¿½è¸ªå™¨

        Args:
            total_items: æ€»é¡¹ç›®æ•°é‡
        """
        self.total_items = total_items
        self.completed_items = 0
        self.start_time = time.time()
        self.item_times = []  # è®°å½•æ¯ä¸ªé¡¹ç›®çš„å¤„ç†æ—¶é—´

    def update(self, completed: int):
        """æ›´æ–°å®Œæˆæ•°é‡"""
        if completed > self.completed_items:
            current_time = time.time()
            # è®°å½•å•ä¸ªé¡¹ç›®å¤„ç†æ—¶é—´
            if self.completed_items > 0:
                time_per_item = (current_time - self.start_time) / completed
                self.item_times.append(time_per_item)
                # åªä¿ç•™æœ€è¿‘10ä¸ªé¡¹ç›®çš„æ—¶é—´ï¼Œç”¨äºåŠ¨æ€è°ƒæ•´é¢„ä¼°
                if len(self.item_times) > 10:
                    self.item_times.pop(0)

        self.completed_items = completed

    def get_progress(self) -> float:
        """è·å–è¿›åº¦ç™¾åˆ†æ¯” (0.0 - 1.0)"""
        if self.total_items == 0:
            return 0.0
        return min(self.completed_items / self.total_items, 1.0)

    def get_eta(self) -> Optional[str]:
        """è·å–é¢„è®¡å‰©ä½™æ—¶é—´ï¼ˆæ ¼å¼åŒ–å­—ç¬¦ä¸²ï¼‰"""
        if self.completed_items == 0 or self.total_items == 0:
            return None

        elapsed = time.time() - self.start_time
        remaining_items = self.total_items - self.completed_items

        if remaining_items <= 0:
            return "å®Œæˆ"

        # ä½¿ç”¨æœ€è¿‘çš„å¤„ç†æ—¶é—´æ¥é¢„ä¼°
        if self.item_times:
            avg_time_per_item = sum(self.item_times) / len(self.item_times)
        else:
            avg_time_per_item = elapsed / self.completed_items

        eta_seconds = remaining_items * avg_time_per_item

        # æ ¼å¼åŒ–æ—¶é—´
        if eta_seconds < 60:
            return f"{int(eta_seconds)}ç§’"
        elif eta_seconds < 3600:
            return f"{int(eta_seconds / 60)}åˆ†{int(eta_seconds % 60)}ç§’"
        else:
            hours = int(eta_seconds / 3600)
            minutes = int((eta_seconds % 3600) / 60)
            return f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ"

    def get_elapsed(self) -> str:
        """è·å–å·²ç”¨æ—¶é—´ï¼ˆæ ¼å¼åŒ–å­—ç¬¦ä¸²ï¼‰"""
        elapsed = time.time() - self.start_time
        if elapsed < 60:
            return f"{int(elapsed)}ç§’"
        elif elapsed < 3600:
            return f"{int(elapsed / 60)}åˆ†{int(elapsed % 60)}ç§’"
        else:
            hours = int(elapsed / 3600)
            minutes = int((elapsed % 3600) / 60)
            return f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ"


class EvaluationProcessInterface:
    """é‡æ„åçš„è¯„ä¼°è¿‡ç¨‹ç•Œé¢

    æ”¹è¿›:
    - å‡†ç¡®çš„è¿›åº¦è®¡ç®—
    - é¢„è®¡å‰©ä½™æ—¶é—´æ˜¾ç¤º
    - æ›´æµç•…çš„UIæ›´æ–°
    """

    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°è¿‡ç¨‹ç•Œé¢"""
        pass
    
    def render(self):
        """æ¸²æŸ“è¯„ä¼°è¿‡ç¨‹ç•Œé¢"""
        st.header("âš™ï¸ è¯„ä¼°è¿‡ç¨‹")

        # æ£€æŸ¥æ˜¯å¦æœ‰è¯„ä¼°å‚æ•°
        if not hasattr(st.session_state, 'evaluation_params') or st.session_state.evaluation_params is None:
            st.warning("âš ï¸ è¯·å…ˆåœ¨æ–‡ä»¶ä¸Šä¼ ç•Œé¢é…ç½®è¯„ä¼°å‚æ•°")
            st.info("ğŸ’¡ è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š")
            st.info("1ï¸âƒ£ åˆ‡æ¢åˆ°ã€Œä¿¡æ¯å¡«å†™ã€é€‰é¡¹å¡å¡«å†™åŸºæœ¬ä¿¡æ¯")
            st.info("2ï¸âƒ£ åˆ‡æ¢åˆ°ã€Œæ–‡ä»¶ä¸Šä¼ ã€é€‰é¡¹å¡ä¸Šä¼ æ–‡ä»¶å¹¶é…ç½®å‚æ•°")
            st.info("3ï¸âƒ£ ç‚¹å‡»ã€Œå¼€å§‹æµ‹è¯„ã€æŒ‰é’®")
            return

        # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦å­˜åœ¨
        if not st.session_state.get('current_timestamp'):
            st.warning("âš ï¸ æ—¶é—´æˆ³æœªåˆå§‹åŒ–ï¼Œè¯·é‡æ–°å¼€å§‹è¯„ä¼°æµç¨‹")
            st.info("ğŸ’¡ è¯·è¿”å›ã€Œæ–‡ä»¶ä¸Šä¼ ã€é€‰é¡¹å¡é‡æ–°é…ç½®")
            return

        # æ£€æŸ¥è¯„ä¼°çŠ¶æ€
        if st.session_state.get('evaluation_running', False):
            self._run_evaluation_process()
        elif st.session_state.get('evaluation_completed', False):
            st.success("ğŸ‰ è¯„ä¼°ä»»åŠ¡å·²æˆåŠŸå®Œæˆï¼")
            st.info("ğŸ“Š è¯·åˆ‡æ¢åˆ°ã€Œç»“æœåˆ†æã€é€‰é¡¹å¡æŸ¥çœ‹è¯¦ç»†ç»“æœ")
        else:
            st.info("ğŸ“‹ è¯·åœ¨æ–‡ä»¶ä¸Šä¼ ç•Œé¢ç‚¹å‡»'å¼€å§‹æµ‹è¯„'æŒ‰é’®å¼€å§‹è¯„ä¼°")
            self._display_evaluation_params()
    
    def _display_evaluation_params(self):
        """æ˜¾ç¤ºè¯„ä¼°å‚æ•°"""
        params = st.session_state.evaluation_params

        # æ£€æŸ¥å‚æ•°æ˜¯å¦å­˜åœ¨
        if not params:
            st.info("ğŸ’¡ è¯„ä¼°å‚æ•°å°šæœªé…ç½®")
            return

        with st.expander("ğŸ“Š è¯„ä¼°é…ç½®è¯¦æƒ…", expanded=True):
            col1, col2 = st.columns(2)

            with col1:
                st.write("**åŸºæœ¬é…ç½®:**")
                st.write(f"â€¢ æ¨¡å‹: {params.get('model_name', 'N/A')}")
                st.write(f"â€¢ æ–‡ä»¶æ•°é‡: {params.get('file_count', 0)}")
                st.write(f"â€¢ è¯„ä¼°è½®æ¬¡: {params.get('evaluation_rounds', 1)}")

            with col2:
                st.write("**é˜ˆå€¼è®¾ç½®:**")
                st.write(f"â€¢ Stage1 ç­”æ¡ˆé˜ˆå€¼: {params.get('stage1_answer_threshold', 0.7)}")
                st.write(f"â€¢ Stage1 æ¨ç†é˜ˆå€¼: {params.get('stage1_reasoning_threshold', 0.7)}")
                st.write(f"â€¢ Stage2 ç­”æ¡ˆé˜ˆå€¼: {params.get('stage2_answer_threshold', 0.7)}")
                st.write(f"â€¢ Stage2 æ¨ç†é˜ˆå€¼: {params.get('stage2_reasoning_threshold', 0.7)}")

            st.write("**æ–‡ä»¶åˆ—è¡¨:**")
            file_paths = params.get('file_paths', [])
            if file_paths:
                for i, file_path in enumerate(file_paths, 1):
                    st.write(f"{i}. `{Path(file_path).name}`")
            else:
                st.write("â€¢ æš‚æ— æ–‡ä»¶")
    
    def _run_evaluation_process(self):
        """è¿è¡Œè¯„ä¼°è¿‡ç¨‹"""
        params = st.session_state.evaluation_params
        
        st.subheader("ğŸ”„ è¯„ä¼°è¿›è¡Œä¸­...")
        
        # åˆ›å»ºè¿›åº¦æ˜¾ç¤ºå®¹å™¨
        progress_container = st.container()
        log_container = st.container()
        
        with progress_container:
            st.write("**ğŸ“Š æ€»ä½“è¿›åº¦ (æ–‡ä»¶çº§åˆ«çš„å®è§‚è¿›åº¦)**")
            overall_progress = st.progress(0)
            overall_status = st.empty()
            
            st.write("**ğŸ”„ é—®é¢˜å¤„ç†è¿›åº¦ (æ¯ä¸ªé—®é¢˜çš„è¯¦ç»†å¤„ç†è¿‡ç¨‹)**")
            question_progress = st.progress(0)
            question_status = st.empty()
        
        with log_container:
            st.markdown("### ğŸ“‹ è¯„ä¼°æ—¥å¿—")
            log_placeholder = st.empty()
            
            # æ·»åŠ æ—¥å¿—è¯´æ˜
            st.caption("ğŸ’¡ æ˜¾ç¤ºå…³é”®è¿›åº¦ä¿¡æ¯ï¼šæ–‡ä»¶å¤„ç†ã€é˜¶æ®µå®Œæˆã€é—®é¢˜ç»Ÿè®¡ç­‰")
        
        # æ‰§è¡Œè¯„ä¼°
        try:
            self._execute_evaluation_workflow(
                params, overall_progress, overall_status, 
                question_progress, question_status, log_placeholder
            )
        except Exception as e:
            st.error(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            st.session_state.evaluation_running = False
    
    def _execute_evaluation_workflow(self, params, overall_progress, overall_status, 
                                   question_progress, question_status, log_placeholder):
        """æ‰§è¡Œè¯„ä¼°å·¥ä½œæµ"""
        logs = []
        total_files = len(params['file_paths'])
        
        # è¿›åº¦æƒé‡åˆ†é… (åŸºäºå®é™…æ—¶é—´åˆ†å¸ƒ)
        INIT_WEIGHT = 0.05      # åˆå§‹åŒ–: 5%
        STAGE1_WEIGHT = 0.50    # Stage1: 50% (æ•°æ®å¤„ç†ã€LLMæµ‹è¯•ã€è¯„ä¼°)
        STAGE2_WEIGHT = 0.30    # Stage2: 30% (æ·±åº¦è¯„ä¼°)
        RESULT_WEIGHT = 0.10    # ç»“æœå¤„ç†: 10%
        VISUAL_WEIGHT = 0.05    # å¯è§†åŒ–: 5%
        
        # æ—¥å¿—æ›´æ–°è®¡æ•°å™¨ï¼Œç”¨äºæ§åˆ¶æ›´æ–°é¢‘ç‡
        log_update_counter = [0]
        
        def log_message(message, log_type="INFO"):
            """è®°å½•æ—¥å¿—æ¶ˆæ¯"""
            try:
                timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
                
                # æ ¹æ®æ—¥å¿—ç±»å‹æ·»åŠ å›¾æ ‡
                if log_type == "ERROR":
                    icon = "âŒ"
                elif log_type == "WARNING":
                    icon = "âš ï¸"
                elif log_type == "SUCCESS":
                    icon = "âœ…"
                else:
                    icon = "â„¹ï¸"
                
                log_entry = f"{timestamp} {icon} {message}"
                logs.append(log_entry)
                
                # åªä¿ç•™æœ€è¿‘25æ¡æ—¥å¿—ä»¥é¿å…ç•Œé¢è¿‡äºæ‹¥æŒ¤
                recent_logs = logs[-25:]
                
                # æ§åˆ¶æ›´æ–°é¢‘ç‡ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„ç•Œé¢åˆ·æ–°
                log_update_counter[0] += 1
                if log_update_counter[0] % 2 == 0 or log_type == "ERROR":  # æ¯2æ¡æ›´æ–°ä¸€æ¬¡ï¼Œé”™è¯¯ç«‹å³æ˜¾ç¤º
                    log_text = "\n".join(recent_logs)
                    
                    # ä½¿ç”¨markdownæ˜¾ç¤ºæ—¥å¿—ï¼Œæ”¯æŒæ›´å¥½çš„æ ¼å¼
                    with log_placeholder.container():
                        st.text(log_text)
                    
            except Exception as e:
                # å¦‚æœæ—¥å¿—æ˜¾ç¤ºå‡ºé”™ï¼Œè‡³å°‘ä¸è¦å½±å“ä¸»æµç¨‹
                pass
        
        # åˆ›å»ºæ€»ä½“è¿›åº¦è¿½è¸ªå™¨
        overall_tracker = ProgressTracker(total_files * params['evaluation_rounds'])

        def update_overall_progress(progress_ratio, stage, file_index=0, file_name="", process_info="", current_round=None, total_rounds=None):
            """æ›´æ–°æ€»ä½“è¿›åº¦æ¡ - æ˜¾ç¤ºæ–‡ä»¶çº§åˆ«çš„å®è§‚è¿›åº¦ï¼ˆå«é¢„è®¡æ—¶é—´ï¼‰"""
            # æ›´æ–°è¿›åº¦æ¡
            overall_progress.progress(min(progress_ratio, 1.0))

            # æ›´æ–°è¿½è¸ªå™¨
            completed_count = int(progress_ratio * total_files * params['evaluation_rounds'])
            overall_tracker.update(completed_count)

            # æ„å»ºçŠ¶æ€æ–‡æœ¬ - é‡ç‚¹æ˜¾ç¤ºé˜¶æ®µå’Œæ–‡ä»¶å¤„ç†æ•°
            percentage = int(progress_ratio * 100)

            # æ ¹æ®é˜¶æ®µç±»å‹æ˜¾ç¤ºä¸åŒçš„å›¾æ ‡å’Œæè¿°
            if "Stage1" in stage:
                stage_icon = "1ï¸âƒ£"
                stage_desc = "åŸºç¡€é—®ç­”è¯„ä¼°"
            elif "Stage2" in stage:
                stage_icon = "2ï¸âƒ£"
                stage_desc = "æ·±åº¦æ¨ç†è¯„ä¼°"
            elif "åˆå§‹åŒ–" in stage:
                stage_icon = "âš™ï¸"
                stage_desc = "åˆå§‹åŒ–"
            elif "ç»“æœå¤„ç†" in stage:
                stage_icon = "ğŸ“‹"
                stage_desc = "ç»“æœå¤„ç†"
            elif "å¯è§†åŒ–" in stage:
                stage_icon = "ğŸ“Š"
                stage_desc = "å¯è§†åŒ–ç”Ÿæˆ"
            else:
                stage_icon = "ğŸ”„"
                stage_desc = stage

            # æ„å»ºçŠ¶æ€æ–‡æœ¬ - æ˜¾ç¤ºæ–‡ä»¶å¤„ç†æ•°ã€è¯„ä¼°è½®æ¬¡å’Œæ–‡ä»¶å
            if current_round and total_rounds:
                rounds_display = f"è¯„ä¼°è½®æ¬¡ {current_round}/{total_rounds}"
            else:
                rounds_display = f"è¯„ä¼°è½®æ¬¡ {params['evaluation_rounds']}"

            # è·å–æ—¶é—´ä¿¡æ¯
            elapsed = overall_tracker.get_elapsed()
            eta = overall_tracker.get_eta()

            if file_index > 0 and file_name:
                file_display = f"æ–‡ä»¶ {file_index}/{total_files}"
                status_text = f"{stage_icon} æ€»ä½“è¿›åº¦ ({percentage}%) | {stage_desc} | {file_display} | {rounds_display}"
            elif file_name and not file_index:
                # å¯¹äºç³»ç»Ÿçº§æ“ä½œï¼Œåªæ˜¾ç¤ºæ“ä½œç±»å‹å’Œè¯„ä¼°è½®æ¬¡
                status_text = f"{stage_icon} æ€»ä½“è¿›åº¦ ({percentage}%) | {stage_desc} | {rounds_display}"
            else:
                # å¯¹äºåˆå§‹åŒ–ç­‰é˜¶æ®µï¼Œä¹Ÿæ˜¾ç¤ºè¯„ä¼°è½®æ¬¡ä¿¡æ¯
                status_text = f"{stage_icon} æ€»ä½“è¿›åº¦ ({percentage}%) | {stage_desc} | {rounds_display}"

            # æ·»åŠ æ—¶é—´ä¿¡æ¯
            if eta and eta != "å®Œæˆ":
                status_text += f" | â±ï¸ å·²ç”¨: {elapsed} | é¢„è®¡å‰©ä½™: {eta}"
            else:
                status_text += f" | â±ï¸ å·²ç”¨: {elapsed}"

            overall_status.text(status_text)
        
        def create_question_progress_tracker(stage_name, file_name=""):
            """åˆ›å»ºé—®é¢˜çº§åˆ«çš„è¿›åº¦è·Ÿè¸ªå™¨ - ç²¾ç¡®æ˜¾ç¤ºæ¯ä¸ªé—®é¢˜çš„å¤„ç†è¿›åº¦"""
            def callback(current, total, question_id, process_type="", current_round=1, total_rounds=1):
                if total > 0:
                    progress_ratio = min(current / total, 1.0)
                    percentage = int(progress_ratio * 100)
                    
                    # æ ¹æ®è¿‡ç¨‹ç±»å‹æ˜¾ç¤ºä¸åŒçš„ä¿¡æ¯
                    if process_type == "testing":
                        if "Stage1" in stage_name:
                            process_icon = "ğŸ¤–"
                            process_desc = "åŸºç¡€æµ‹è¯•"
                            process_detail = "ä»…ä½¿ç”¨é—®é¢˜æ–‡æœ¬"
                        elif "Stage2" in stage_name:
                            process_icon = "ğŸ”„"
                            process_desc = "æ·±åº¦é‡æµ‹"
                            process_detail = "ä½¿ç”¨é—®é¢˜+å†…å®¹ä¸Šä¸‹æ–‡"
                        else:
                            process_icon = "ğŸ¤–"
                            process_desc = "æµ‹è¯•è¿‡ç¨‹"
                            process_detail = ""
                        
                        if question_id:
                            status_text = f"{process_icon} {process_desc} | æ–‡ä»¶: {file_name} | é—®é¢˜ {current}/{total} ({percentage}%) | ID: {question_id}"
                            if process_detail:
                                status_text += f" | {process_detail}"
                        else:
                            status_text = f"{process_icon} {process_desc} | æ–‡ä»¶: {file_name} | {current}/{total} ({percentage}%)"
                            
                    elif process_type == "evaluating":
                        if "Stage1" in stage_name:
                            process_icon = "ğŸ“Š"
                            process_desc = "åŸºç¡€è¯„ä¼°"
                            process_detail = "è¯„ä¼°åŸºç¡€å›ç­”è´¨é‡"
                        elif "Stage2" in stage_name:
                            process_icon = "ğŸ“‹"
                            process_desc = "æ·±åº¦è¯„ä¼°"
                            process_detail = "è¯„ä¼°æ·±åº¦æ¨ç†èƒ½åŠ›"
                        else:
                            process_icon = "ğŸ“Š"
                            process_desc = "è¯„ä¼°è¿‡ç¨‹"
                            process_detail = ""
                        
                        if question_id:
                            status_text = f"{process_icon} {process_desc} | æ–‡ä»¶: {file_name} | é—®é¢˜ {current}/{total} ({percentage}%) | ID: {question_id}"
                            if process_detail:
                                status_text += f" | {process_detail}"
                        else:
                            status_text = f"{process_icon} {process_desc} | æ–‡ä»¶: {file_name} | {current}/{total} ({percentage}%)"
                            
                    else:
                        # å…¶ä»–è¿‡ç¨‹ï¼ˆæ•°æ®å¤„ç†ã€åˆå§‹åŒ–ç­‰ï¼‰
                        process_icon = "âš™ï¸"
                        if question_id:
                            status_text = f"{process_icon} {stage_name} | æ–‡ä»¶: {file_name} | é¡¹ç›® {current}/{total} ({percentage}%) | ID: {question_id}"
                        else:
                            status_text = f"{process_icon} {stage_name} | æ–‡ä»¶: {file_name} | {current}/{total} ({percentage}%)"
                        
                        if process_type:
                            status_text += f" | {process_type}"
                    
                    # æ›´æ–°è¿›åº¦æ¡å’ŒçŠ¶æ€
                    question_progress.progress(progress_ratio)
                    question_status.text(status_text)
                else:
                    question_progress.progress(0)
                    question_status.text(f"â³ {stage_name} | æ–‡ä»¶: {file_name} | å‡†å¤‡ä¸­...")
            
            return callback
        
        try:
            # === 1. åˆå§‹åŒ–é˜¶æ®µ ===
            log_message("ğŸš€ å¼€å§‹è¯„ä¼°ä»»åŠ¡")
            update_overall_progress(0.01, "åˆå§‹åŒ–")
            
            # ç¡®ä¿ä½¿ç”¨ä¸æ–‡ä»¶ä¸Šä¼ ä¸€è‡´çš„æ—¶é—´æˆ³
            from utils.file_manager_singleton import ensure_timestamp_consistency
            session_timestamp = st.session_state.get('current_timestamp')
            if session_timestamp:
                ensure_timestamp_consistency(params['model_name'], session_timestamp)
            else:
                # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³ï¼Œè¯´æ˜æœ‰é—®é¢˜ï¼Œéœ€è¦ç”Ÿæˆä¸€ä¸ª
                reset_file_manager_for_new_test(params['model_name'])
            
            # åˆ›å»ºåˆå§‹åŒ–è¿›åº¦å›è°ƒ
            init_callback = create_question_progress_tracker("åˆå§‹åŒ–è¯„ä¼°å™¨", "ç³»ç»Ÿ")
            init_callback(0, 3, "", "å‡†å¤‡ä¸­")
            
            # è·å–è¯„ä¼°æ¨¡å‹åç§°
            eval_model_name = params.get('eval_model_name', 'siliconflow_deepseek_v3')

            stage1_evaluator = Stage1Evaluator(
                model_name=params['model_name'],
                eval_model_name=eval_model_name
            )
            init_callback(1, 3, "", "Stage1è¯„ä¼°å™¨å®Œæˆ")

            stage2_evaluator = Stage2Evaluator(
                model_name=params['model_name'],
                eval_model_name=eval_model_name
            )
            init_callback(2, 3, "", "Stage2è¯„ä¼°å™¨å®Œæˆ")
            
            init_callback(3, 3, "", "æ—¶é—´æˆ³ç›®å½•å·²ç”Ÿæˆ")
            
            log_message(f"ğŸ“‹ å‡†å¤‡å¤„ç† {total_files} ä¸ªæ–‡ä»¶ï¼Œå…± {params['evaluation_rounds']} è½®è¯„ä¼°")
            
            update_overall_progress(INIT_WEIGHT, "åˆå§‹åŒ–å®Œæˆ")
            
            # === 2. Stage1 è¯„ä¼°é˜¶æ®µ ===
            log_message("1ï¸âƒ£ å¼€å§‹Stage1åŸºç¡€é—®ç­”è¯„ä¼°")
            stage1_results = []
            total_stage1_questions = 0
            total_need_retest = 0
            
            for i, file_path in enumerate(params['file_paths'], 1):
                file_name = Path(file_path).stem
                stage1_progress_base = INIT_WEIGHT + (STAGE1_WEIGHT * (i - 1) / total_files)
                stage1_progress_step = STAGE1_WEIGHT / total_files
                
                log_message(f"ğŸ“„ æ–‡ä»¶ {i}/{total_files}: {file_name}")
                update_overall_progress(
                    stage1_progress_base, 
                    "Stage1 åŸºç¡€é—®ç­”è¯„ä¼°", 
                    file_index=i,
                    file_name=file_name,
                    process_info="æ•°æ®å¤„ç†"
                )
                
                # æ‰§è¡ŒStage1è¯„ä¼°å¹¶è·Ÿè¸ªè¿›åº¦
                stage1_result = self._run_stage1_with_progress(
                    stage1_evaluator, file_path, params, i, file_name, 
                    create_question_progress_tracker, log_message, update_overall_progress,
                    stage1_progress_base, stage1_progress_step
                )
                stage1_results.append(stage1_result)
                
                # ä»ç»“æœä¸­è·å–ç»Ÿè®¡ä¿¡æ¯
                # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šè½®è¯„ä¼°ç»“æœ
                if 'aggregated_statistics' in stage1_result:
                    # å¤šè½®è¯„ä¼°ï¼šä½¿ç”¨æ±‡æ€»ç»Ÿè®¡æ•°æ®
                    stats = stage1_result.get('aggregated_statistics', {})
                    total_questions = stats.get('total_questions', 0)
                    need_retest = stats.get('avg_need_retest', 0)
                    correct_answers = stats.get('avg_correct_answers', 0)
                    reasoning_errors = stats.get('avg_reasoning_errors', 0)
                else:
                    # å•è½®è¯„ä¼°ï¼šä½¿ç”¨å¸¸è§„ç»Ÿè®¡æ•°æ®
                    stats = stage1_result.get('statistics', {})
                    total_questions = stats.get('total_questions', 0)
                    need_retest = stats.get('need_retest', 0)
                    correct_answers = stats.get('correct_answers', 0)
                    reasoning_errors = stats.get('reasoning_errors', 0)
                
                total_stage1_questions += total_questions
                total_need_retest += need_retest
                
                log_message(f"âœ… {file_name}: å®Œæˆ {total_questions} é¢˜ï¼Œéœ€é‡æµ‹ {need_retest} é¢˜")
                
                update_overall_progress(
                    stage1_progress_base + stage1_progress_step, 
                    "Stage1 åŸºç¡€é—®ç­”è¯„ä¼°", 
                    file_index=i,
                    file_name=file_name,
                    process_info="å®Œæˆ"
                )
            
            log_message(f"1ï¸âƒ£ Stage1å®Œæˆ: æ€»è®¡ {total_stage1_questions} é¢˜ï¼Œéœ€é‡æµ‹ {total_need_retest} é¢˜")
            
            # === 3. Stage2 è¯„ä¼°é˜¶æ®µ ===
            log_message("2ï¸âƒ£ å¼€å§‹Stage2æ·±åº¦æ¨ç†è¯„ä¼°")
            total_stage2_questions = 0
            
            for i, file_path in enumerate(params['file_paths'], 1):
                file_name = Path(file_path).stem
                stage2_progress_base = INIT_WEIGHT + STAGE1_WEIGHT + (STAGE2_WEIGHT * (i - 1) / total_files)
                stage2_progress_step = STAGE2_WEIGHT / total_files
                
                update_overall_progress(
                    stage2_progress_base, 
                    "Stage2 æ·±åº¦æ¨ç†è¯„ä¼°", 
                    file_index=i,
                    file_name=file_name,
                    process_info="æ£€æŸ¥éœ€æ±‚"
                )
                
                # æ‰§è¡Œå¤šè½®Stage2è¯„ä¼°
                file_stage2_questions = self._run_multi_round_stage2(
                    stage2_evaluator, file_name, params, i, 
                    create_question_progress_tracker, log_message,
                    update_overall_progress, stage2_progress_base, stage2_progress_step
                )
                
                total_stage2_questions += file_stage2_questions
                
                update_overall_progress(
                    stage2_progress_base + stage2_progress_step, 
                    "Stage2 æ·±åº¦æ¨ç†è¯„ä¼°", 
                    file_index=i,
                    file_name=file_name,
                    process_info="å®Œæˆ"
                )
            
            if total_stage2_questions > 0:
                log_message(f"2ï¸âƒ£ Stage2å®Œæˆ: æ€»è®¡å¤„ç† {total_stage2_questions} é¢˜")
            else:
                log_message("2ï¸âƒ£ Stage2å®Œæˆ: æ‰€æœ‰é¢˜ç›®å‡é€šè¿‡Stage1")
            
            # === 4. ç»“æœå¤„ç†é˜¶æ®µ ===
            log_message("ğŸ“Š å¼€å§‹åˆ†æè¯„æµ‹ç»“æœ")
            result_progress_base = INIT_WEIGHT + STAGE1_WEIGHT + STAGE2_WEIGHT
            
            processor = ResultProcessor()
            file_names = [Path(fp).stem for fp in params['file_paths']]
            
            # å¤„ç†å•æ–‡ä»¶ç»“æœ
            result_callback = create_question_progress_tracker("ç»“æœå¤„ç†", "ç³»ç»Ÿ")
            for i, file_name in enumerate(file_names, 1):
                result_progress = result_progress_base + (RESULT_WEIGHT * i / (total_files + 1))
                update_overall_progress(
                    result_progress, 
                    "ç»“æœå¤„ç†ä¸åˆ†æ", 
                    file_index=i,
                    file_name=file_name,
                    process_info="å•æ–‡ä»¶åˆ†æ"
                )
                
                try:
                    # å¤„ç†æ­¥éª¤è¿›åº¦
                    result_callback(0, 3, file_name, "åŠ è½½Stage1ç»“æœ")
                    result_callback(1, 3, file_name, "åŠ è½½Stage2ç»“æœ")
                    result_callback(2, 3, file_name, "åˆå¹¶åˆ†æ")
                    
                    processor.process_single_file_results(params['model_name'], file_name)
                    
                    result_callback(3, 3, file_name, "âœ“ å®Œæˆ")
                    log_message(f"âœ… {file_name}: ç»“æœåˆ†æå®Œæˆ")
                except Exception as e:
                    log_message(f"âŒ {file_name}: ç»“æœå¤„ç†å¤±è´¥", "ERROR")
                    result_callback(3, 3, file_name, "âŒ å¤±è´¥")
            
            # å¤šæ–‡ä»¶æ±‡æ€»
            enable_multi_file = len(file_names) > 1
            if enable_multi_file:
                summary_callback = create_question_progress_tracker("å¤šæ–‡ä»¶æ±‡æ€»", "ç³»ç»Ÿ")
                
                try:
                    summary_callback(0, 3, "", "æ”¶é›†æ•°æ®")
                    summary_callback(1, 3, "", "ç”Ÿæˆç»Ÿè®¡")
                    summary_callback(2, 3, "", "ç”ŸæˆæŠ¥å‘Š")
                    
                    processor.process_multi_file_results(params['model_name'], enable_multi_file)
                    
                    summary_callback(3, 3, "", "âœ“ æ±‡æ€»å®Œæˆ")
                    log_message("ğŸ“‹ å¤šæ–‡ä»¶æ±‡æ€»å®Œæˆ")
                except Exception as e:
                    log_message("âŒ å¤šæ–‡ä»¶æ±‡æ€»å¤±è´¥", "ERROR")
                    summary_callback(3, 3, "", "âŒ æ±‡æ€»å¤±è´¥")
            else:
                skip_summary_callback = create_question_progress_tracker("å¤šæ–‡ä»¶æ±‡æ€»", "ç³»ç»Ÿ")
                skip_summary_callback(1, 1, "", "å•æ–‡ä»¶æ¨¡å¼ï¼Œè·³è¿‡")
            
            # === 5. å¯è§†åŒ–é˜¶æ®µ ===
            log_message("ğŸ“ˆ å¼€å§‹ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
            visual_progress_base = INIT_WEIGHT + STAGE1_WEIGHT + STAGE2_WEIGHT + RESULT_WEIGHT
            update_overall_progress(visual_progress_base, "å¯è§†åŒ–ç”Ÿæˆ")
            
            try:
                visualizer = DataVisualizer()
                visual_callback = create_question_progress_tracker("å¯è§†åŒ–ç”Ÿæˆ", "ç³»ç»Ÿ")
                
                # ç”Ÿæˆå›¾è¡¨
                chart_steps = len(file_names) * 2 + (2 if len(file_names) > 1 else 0)
                current_step = 0
                
                # ç”Ÿæˆå•æ–‡ä»¶å›¾è¡¨
                for i, file_name in enumerate(file_names, 1):
                    visual_progress = visual_progress_base + (VISUAL_WEIGHT * 0.8 * i / len(file_names))
                    update_overall_progress(
                        visual_progress, 
                        "å¯è§†åŒ–ç”Ÿæˆ", 
                        file_index=i,
                        file_name=file_name
                    )
                    
                    # æŸ±çŠ¶å›¾
                    current_step += 1
                    visual_callback(current_step, chart_steps, file_name, "ç”ŸæˆæŸ±çŠ¶å›¾")
                    
                    # é¥¼å›¾
                    current_step += 1
                    visual_callback(current_step, chart_steps, file_name, "ç”Ÿæˆé¥¼å›¾")
                
                # å®é™…ç”Ÿæˆæ‰€æœ‰å›¾è¡¨
                visualization_results = visualizer.visualize_files(params['model_name'], file_names)
                
                # å¤šæ–‡ä»¶æ±‡æ€»å›¾è¡¨
                if len(file_names) > 1:
                    update_overall_progress(
                        visual_progress_base + VISUAL_WEIGHT * 0.9, 
                        "å¯è§†åŒ–ç”Ÿæˆ", 
                        file_name="å¤šæ–‡ä»¶æ±‡æ€»"
                    )
                    
                    current_step += 1
                    visual_callback(current_step, chart_steps, "æ±‡æ€»", "ç”Ÿæˆæ±‡æ€»æŸ±çŠ¶å›¾")
                    current_step += 1
                    visual_callback(current_step, chart_steps, "æ±‡æ€»", "ç”Ÿæˆæ±‡æ€»é¥¼å›¾")
                
                visual_callback(chart_steps, chart_steps, "", "âœ“ å¯è§†åŒ–å®Œæˆ")
                
                log_message("ğŸ“Š å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
                
            except Exception as e:
                log_message("âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥", "ERROR")
                error_callback = create_question_progress_tracker("å¯è§†åŒ–ç”Ÿæˆ", "ç³»ç»Ÿ")
                error_callback(1, 1, "", "âŒ å›¾è¡¨ç”Ÿæˆå¤±è´¥")
                visualization_results = None
            
            # === 6. å®Œæˆ ===
            update_overall_progress(1.0, "âœ… è¯„ä¼°å®Œæˆ")
            complete_callback = create_question_progress_tracker("è¯„ä¼°å®Œæˆ", "ç³»ç»Ÿ")
            complete_callback(1, 1, "", "ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆ")
            
            # ç”Ÿæˆè¯„ä¼°æ€»ç»“
            log_message("ğŸ‰ è¯„ä¼°ä»»åŠ¡å®Œæˆ", "SUCCESS")
            retest_rate = (total_need_retest/total_stage1_questions*100) if total_stage1_questions > 0 else 0
            log_message(f"ğŸ“Š æ€»è®¡: {total_stage1_questions}é¢˜ | é‡æµ‹: {total_stage2_questions}é¢˜ | é‡æµ‹ç‡: {retest_rate:.1f}%", "SUCCESS")
            log_message("ğŸ“‹ è¯·åˆ‡æ¢åˆ°'ç»“æœåˆ†æ'æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š", "SUCCESS")
            
            # å¼ºåˆ¶åˆ·æ–°æœ€ç»ˆæ—¥å¿—æ˜¾ç¤º
            try:
                final_log_text = "\n".join(logs[-25:])
                with log_placeholder.container():
                    st.text(final_log_text)
            except:
                pass
            
            # ä¿å­˜ç»“æœ
            st.session_state.evaluation_results = {
                'model_name': params['model_name'],
                'file_names': file_names,
                'visualization_results': visualization_results,
                'enable_multi_file': enable_multi_file,
                'summary_stats': {
                    'total_files': total_files,
                    'total_stage1_questions': total_stage1_questions,
                    'total_stage2_questions': total_stage2_questions,
                    'total_need_retest': total_need_retest
                }
            }
            st.session_state.evaluation_completed = True
            st.session_state.evaluation_running = False
            
            st.success("âœ… è¯„ä¼°å®Œæˆï¼è¯·åˆ‡æ¢åˆ° 'ç»“æœåˆ†æ' é€‰é¡¹å¡æŸ¥çœ‹ç»“æœ")
            
        except Exception as e:
            log_message(f"è¯„ä¼°è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}", "ERROR")
            
            # å¼ºåˆ¶åˆ·æ–°é”™è¯¯æ—¥å¿—æ˜¾ç¤º
            try:
                error_log_text = "\n".join(logs[-25:])
                with log_placeholder.container():
                    st.text(error_log_text)
            except:
                pass
                
            update_overall_progress(0, "âŒ è¯„ä¼°å¤±è´¥")
            error_tracker = create_question_progress_tracker("å¤„ç†ä¸­æ–­", "ç³»ç»Ÿ")
            error_tracker(0, 1, "", "âŒ å¤„ç†ä¸­æ–­")
            st.session_state.evaluation_running = False
            raise
    
    def _run_stage1_with_progress(self, stage1_evaluator, file_path, params, file_index, 
                                 file_name, create_question_progress_tracker, log_message, 
                                 update_overall_progress, progress_base, progress_step):
        """æ‰§è¡ŒStage1è¯„ä¼°å¹¶è·Ÿè¸ªè¿›åº¦ - åŒºåˆ†æµ‹è¯•è¿‡ç¨‹å’Œè¯„ä¼°è¿‡ç¨‹"""
        
        # åˆ›å»ºStage1ä¸“ç”¨çš„é—®é¢˜è¿›åº¦è·Ÿè¸ªå™¨ï¼Œèƒ½å¤Ÿä¼ é€’è½®æ¬¡ä¿¡æ¯åˆ°æ€»ä½“è¿›åº¦æ¡
        current_round_ref = [1]  # ä½¿ç”¨åˆ—è¡¨æ¥å­˜å‚¨å½“å‰è½®æ¬¡ï¼Œä¾¿äºåœ¨å›è°ƒä¸­ä¿®æ”¹
        total_rounds_ref = [params['evaluation_rounds']]
        
        def stage1_tracker_with_round_update(current, total, question_id, process_type="", current_round=1, total_rounds=1):
            # æ›´æ–°è½®æ¬¡ä¿¡æ¯
            current_round_ref[0] = current_round
            total_rounds_ref[0] = total_rounds
            
            # è°ƒç”¨åŸå§‹çš„é—®é¢˜è¿›åº¦è·Ÿè¸ªå™¨
            stage1_tracker = create_question_progress_tracker("Stage1", file_name)
            stage1_tracker(current, total, question_id, process_type, current_round, total_rounds)
            
            # å¦‚æœæ˜¯æµ‹è¯•æˆ–è¯„ä¼°è¿‡ç¨‹ï¼Œæ›´æ–°æ€»ä½“è¿›åº¦æ¡ä»¥æ˜¾ç¤ºå½“å‰è½®æ¬¡
            if process_type in ["testing", "evaluating"]:
                current_progress = progress_base + progress_step * 0.5  # ä¼°ç®—å½“å‰è¿›åº¦
                update_overall_progress(
                    current_progress, 
                    "Stage1 åŸºç¡€é—®ç­”è¯„ä¼°", 
                    file_index=file_index,
                    file_name=file_name,
                    process_info=f"{process_type}è¿‡ç¨‹",
                    current_round=current_round,
                    total_rounds=total_rounds
                )
        
        stage1_tracker_with_round_update(0, 1, "", "åˆå§‹åŒ–æ•°æ®å¤„ç†")
        
        try:
            # å…ˆè¯»å–æ–‡ä»¶è·å–é—®é¢˜æ•°é‡
            from utils.excel_processor import ExcelProcessor
            processor = ExcelProcessor(file_path)
            data_list = processor.process_data()
            total_questions = len(data_list)
            
            stage1_tracker_with_round_update(1, 1, "", f"æ•°æ®å¤„ç†å®Œæˆ ({total_questions}ä¸ªé—®é¢˜)")
            
            # æ›´æ–°æ€»ä½“è¿›åº¦ - æ•°æ®å¤„ç†å®Œæˆï¼Œå‡†å¤‡å¼€å§‹æµ‹è¯•è¿‡ç¨‹
            update_overall_progress(
                progress_base + progress_step * 0.1, 
                "Stage1 åŸºç¡€é—®ç­”è¯„ä¼°", 
                file_index=file_index,
                file_name=file_name,
                process_info="ğŸ¤– åŸºç¡€æµ‹è¯•è¿‡ç¨‹"
            )
            
            # è®¾ç½®evaluatorçš„è¿›åº¦å›è°ƒ - è¿™é‡Œä¼šæ¥æ”¶åˆ°testingå’Œevaluatingçš„å›è°ƒ
            stage1_evaluator.set_progress_callback(stage1_tracker_with_round_update)
            
            # æ‰§è¡Œå®Œæ•´è¯„ä¼° - å†…éƒ¨ä¼šè°ƒç”¨æµ‹è¯•è¿‡ç¨‹å’Œè¯„ä¼°è¿‡ç¨‹
            result = stage1_evaluator.run_complete_evaluation(
                file_paths=[file_path],
                num_evaluations=params['evaluation_rounds'],
                answer_threshold=params['stage1_answer_threshold'],
                reasoning_threshold=params['stage1_reasoning_threshold']
            )
            
            # ä¸ºæ¯è½®åˆ›å»ºè½®æ¬¡åˆ†ææ–‡ä»¶ï¼ˆå¦‚æœæ˜¯å¤šè½®è¯„ä¼°ï¼‰
            self._create_round_analysis_files_for_stage1(result, file_name, params)
            
            # ä»ç»“æœä¸­è·å–å¤„ç†çš„æ•°æ®é‡ä¿¡æ¯
            stats = result.get('statistics', {})
            actual_questions = stats.get('total_questions', total_questions)
            need_retest = stats.get('need_retest', 0)
            correct_answers = stats.get('correct_answers', 0)
            reasoning_errors = stats.get('reasoning_errors', 0)
            
            stage1_tracker_with_round_update(actual_questions, actual_questions, "", "âœ“ Stage1å®Œæˆ")
            
            # æ›´æ–°æ€»ä½“è¿›åº¦ - Stage1å®Œæˆ
            update_overall_progress(
                progress_base + progress_step * 0.9, 
                "Stage1 åŸºç¡€é—®ç­”è¯„ä¼°", 
                file_index=file_index,
                file_name=file_name,
                process_info="åŸºç¡€æµ‹è¯•+è¯„ä¼°å®Œæˆ"
            )
            
            return result
            
        except Exception as e:
            log_message(f"âŒ {file_name}: Stage1å¤„ç†å¤±è´¥", "ERROR")
            stage1_tracker_with_round_update(1, 1, "", "âŒ å¤„ç†å¤±è´¥")
            raise
    
    def _run_multi_round_stage2(self, stage2_evaluator, file_name, params, file_index,
                               create_question_progress_tracker, log_message,
                               update_overall_progress, progress_base, progress_step):
        """è¿è¡Œå¤šè½®Stage2è¯„ä¼°
        
        Args:
            stage2_evaluator: Stage2è¯„ä¼°å™¨
            file_name: æ–‡ä»¶å
            params: è¯„ä¼°å‚æ•°
            file_index: æ–‡ä»¶ç´¢å¼•
            create_question_progress_tracker: è¿›åº¦è·Ÿè¸ªå™¨åˆ›å»ºå‡½æ•°
            log_message: æ—¥å¿—è®°å½•å‡½æ•°
            update_overall_progress: æ€»ä½“è¿›åº¦æ›´æ–°å‡½æ•°
            progress_base: è¿›åº¦åŸºå‡†å€¼
            progress_step: è¿›åº¦æ­¥é•¿
            
        Returns:
            int: å¤„ç†çš„é—®é¢˜æ€»æ•°
        """
        total_questions_processed = 0
        
        # å¯¹æ¯ä¸ªè¯„ä¼°è½®æ¬¡æ‰§è¡ŒStage2
        for round_num in range(1, params['evaluation_rounds'] + 1):
            # è¯»å–å½“å‰è½®æ¬¡çš„åˆ†ææ–‡ä»¶
            round_analysis = self._load_round_analysis(file_name, round_num)
            if not round_analysis:
                continue
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦Stage2è¯„ä¼°
            need_retest = round_analysis.get('statistics', {}).get('need_retest', 0)
            
            if need_retest > 0:
                log_message(f"ğŸ”„ {file_name}: ç¬¬{round_num}è½®éœ€é‡æµ‹ {need_retest}é¢˜")
                
                # æ„å»ºé‡æµ‹æ•°æ®æ–‡ä»¶è·¯å¾„
                if round_num == 1:
                    retest_file_name = "stage1_to_stage2_data.csv"
                else:
                    retest_file_name = f"stage1_to_stage2_data_round{round_num}.csv"
                
                # æ‰§è¡ŒStage2è¯„ä¼°
                self._run_single_round_stage2(
                    stage2_evaluator, file_name, round_num, retest_file_name,
                    params, file_index, need_retest, create_question_progress_tracker,
                    log_message, update_overall_progress, progress_base, progress_step
                )
                
                total_questions_processed += need_retest
        
        return total_questions_processed
    
    def _load_round_analysis(self, file_name: str, round_num: int) -> Optional[Dict[str, Any]]:
        """åŠ è½½æŒ‡å®šè½®æ¬¡çš„åˆ†ææ–‡ä»¶
        
        Args:
            file_name: æ–‡ä»¶å
            round_num: è½®æ¬¡ç¼–å·
            
        Returns:
            Dict: åˆ†ææ•°æ®ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨è¿”å›None
        """
        try:
            from utils.file_manager_singleton import get_file_manager
            file_manager = get_file_manager()
            
            # æŸ¥æ‰¾æœ€æ–°çš„æ—¶é—´æˆ³ç›®å½•
            timestamped_dir = file_manager.find_latest_timestamp_dir(st.session_state.evaluation_params['model_name'])
            if not timestamped_dir:
                return None
            
            # æ„å»ºè½®æ¬¡åˆ†ææ–‡ä»¶è·¯å¾„
            round_analysis_path = timestamped_dir / file_name / f"{file_name}_analysis_round_{round_num}.json"
            
            if round_analysis_path.exists():
                import json
                with open(round_analysis_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                return None
                
        except Exception as e:
            return None
    
    def _run_single_round_stage2(self, stage2_evaluator, file_name, round_num, retest_file_name,
                                params, file_index, need_retest_count, create_question_progress_tracker,
                                log_message, update_overall_progress, progress_base, progress_step):
        """è¿è¡Œå•è½®Stage2è¯„ä¼°
        
        Args:
            stage2_evaluator: Stage2è¯„ä¼°å™¨
            file_name: æ–‡ä»¶å
            round_num: è½®æ¬¡ç¼–å·
            retest_file_name: é‡æµ‹æ•°æ®æ–‡ä»¶å
            params: è¯„ä¼°å‚æ•°
            file_index: æ–‡ä»¶ç´¢å¼•
            need_retest_count: éœ€è¦é‡æµ‹çš„é—®é¢˜æ•°
            create_question_progress_tracker: è¿›åº¦è·Ÿè¸ªå™¨åˆ›å»ºå‡½æ•°
            log_message: æ—¥å¿—è®°å½•å‡½æ•°
            update_overall_progress: æ€»ä½“è¿›åº¦æ›´æ–°å‡½æ•°
            progress_base: è¿›åº¦åŸºå‡†å€¼
            progress_step: è¿›åº¦æ­¥é•¿
        """
        try:
            from utils.file_manager_singleton import get_file_manager
            file_manager = get_file_manager()
            
            # æŸ¥æ‰¾æœ€æ–°çš„æ—¶é—´æˆ³ç›®å½•
            timestamped_dir = file_manager.find_latest_timestamp_dir(params['model_name'])
            if not timestamped_dir:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°æ¨¡å‹ {params['model_name']} çš„æ—¶é—´æˆ³ç›®å½•")
            
            # æ„å»ºé‡æµ‹æ•°æ®æ–‡ä»¶è·¯å¾„
            retest_file_path = timestamped_dir / file_name / retest_file_name
            
            if not retest_file_path.exists():
                return
            
            # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
            stage2_tracker = create_question_progress_tracker("Stage2", file_name)
            
            def stage2_tracker_with_round_update(current, total, question_id, process_type="", current_round=1, total_rounds=1):
                # æ›´æ–°è½®æ¬¡ä¿¡æ¯
                stage2_tracker(current, total, question_id, process_type, round_num, params['evaluation_rounds'])
            
            # è®¾ç½®evaluatorçš„è¿›åº¦å›è°ƒ
            stage2_evaluator.set_progress_callback(stage2_tracker_with_round_update)
            
            # æ‰§è¡ŒStage2è¯„ä¼°ï¼ˆå•è½®ï¼‰
            result = stage2_evaluator.run_complete_evaluation(
                file_paths=[str(retest_file_path)],
                num_evaluations=1,  # æ¯è½®åªæ‰§è¡Œä¸€æ¬¡
                answer_threshold=params['stage2_answer_threshold'],
                reasoning_threshold=params['stage2_reasoning_threshold']
            )
            
            # åˆ›å»ºè½®æ¬¡åˆ†ææ–‡ä»¶ï¼ˆStage1 + Stage2ï¼‰
            self._create_combined_round_analysis(file_name, round_num, result)
            
        except Exception as e:
            log_message(f"âŒ {file_name}: ç¬¬{round_num}è½®Stage2å¤±è´¥", "ERROR")
            raise
    
    def _create_combined_round_analysis(self, file_name: str, round_num: int, stage2_result: Dict[str, Any]):
        """åˆ›å»ºåˆå¹¶çš„è½®æ¬¡åˆ†ææ–‡ä»¶ï¼ˆStage1 + Stage2ï¼‰
        
        Args:
            file_name: æ–‡ä»¶å
            round_num: è½®æ¬¡ç¼–å·
            stage2_result: Stage2è¯„ä¼°ç»“æœ
        """
        try:
            from utils.result_processor import ResultProcessor
            from utils.file_manager_singleton import get_file_manager
            
            file_manager = get_file_manager()
            processor = ResultProcessor()
            
            # æŸ¥æ‰¾æœ€æ–°çš„æ—¶é—´æˆ³ç›®å½•
            timestamped_dir = file_manager.find_latest_timestamp_dir(st.session_state.evaluation_params['model_name'])
            if not timestamped_dir:
                return
            
            # åŠ è½½Stage1è½®æ¬¡åˆ†ææ–‡ä»¶
            stage1_round_path = timestamped_dir / file_name / f"{file_name}_analysis_round_{round_num}.json"
            if not stage1_round_path.exists():
                return
            
            import json
            with open(stage1_round_path, 'r', encoding='utf-8') as f:
                stage1_data = json.load(f)
            
            # åˆ›å»ºåˆå¹¶çš„è½®æ¬¡åˆ†ææ–‡ä»¶
            processor.create_round_analysis(
                st.session_state.evaluation_params['model_name'], 
                file_name, 
                round_num, 
                stage1_data, 
                stage2_result
            )
            
        except Exception as e:
            pass  # é™é»˜å¤„ç†é”™è¯¯ï¼Œä¸å½±å“ä¸»æµç¨‹
    
    def _create_round_analysis_files_for_stage1(self, stage1_result: Dict[str, Any], file_name: str, params: Dict[str, Any]):
        """ä¸ºStage1çš„æ¯è½®è¯„ä¼°åˆ›å»ºè½®æ¬¡åˆ†ææ–‡ä»¶
        
        Args:
            stage1_result: Stage1è¯„ä¼°ç»“æœ
            file_name: æ–‡ä»¶å
            params: è¯„ä¼°å‚æ•°
        """
        try:
            from utils.result_processor import ResultProcessor
            from utils.file_manager_singleton import get_file_manager
            
            processor = ResultProcessor()
            file_manager = get_file_manager()
            
            # æŸ¥æ‰¾æœ€æ–°çš„æ—¶é—´æˆ³ç›®å½•
            timestamped_dir = file_manager.find_latest_timestamp_dir(params['model_name'])
            if not timestamped_dir:
                return
            
            base_dir = timestamped_dir / file_name
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šè½®è¯„ä¼°
            is_multi_round = stage1_result.get("evaluation_rounds", 0) > 1
            
            if is_multi_round:
                # å¤šè½®è¯„ä¼°ï¼šä¸ºæ¯è½®åˆ›å»ºè½®æ¬¡åˆ†ææ–‡ä»¶
                individual_results = stage1_result.get("individual_results", [])
                for round_result in individual_results:
                    round_number = round_result.get("round_number", 1)
                    
                    # åˆ›å»ºè½®æ¬¡åˆ†ææ–‡ä»¶ï¼ˆåªæœ‰Stage1æ•°æ®ï¼‰
                    processor.create_round_analysis(
                        params['model_name'], 
                        file_name, 
                        round_number, 
                        round_result, 
                        None  # æš‚æ—¶æ²¡æœ‰Stage2æ•°æ®
                    )
            else:
                # å•è½®è¯„ä¼°ï¼šåˆ›å»ºç¬¬1è½®çš„è½®æ¬¡åˆ†ææ–‡ä»¶
                processor.create_round_analysis(
                    params['model_name'], 
                    file_name, 
                    1, 
                    stage1_result, 
                    None  # æš‚æ—¶æ²¡æœ‰Stage2æ•°æ®
                )
                
        except Exception as e:
            pass  # é™é»˜å¤„ç†é”™è¯¯ï¼Œä¸å½±å“ä¸»æµç¨‹
    
    def _run_stage2_with_progress(self, stage2_evaluator, stage1_evaluator, file_name, 
                                 params, file_index, need_retest_count, create_question_progress_tracker, 
                                 log_message, update_overall_progress, progress_base, progress_step):
        """æ‰§è¡ŒStage2è¯„ä¼°å¹¶è·Ÿè¸ªè¿›åº¦ - åŒºåˆ†æµ‹è¯•è¿‡ç¨‹å’Œè¯„ä¼°è¿‡ç¨‹"""
        stage1_to_stage2_path = stage1_evaluator.get_retest_data_path(file_name)
        
        # åˆ›å»ºStage2ä¸“ç”¨çš„é—®é¢˜è¿›åº¦è·Ÿè¸ªå™¨ï¼Œèƒ½å¤Ÿä¼ é€’è½®æ¬¡ä¿¡æ¯åˆ°æ€»ä½“è¿›åº¦æ¡
        current_round_ref = [1]  # ä½¿ç”¨åˆ—è¡¨æ¥å­˜å‚¨å½“å‰è½®æ¬¡ï¼Œä¾¿äºåœ¨å›è°ƒä¸­ä¿®æ”¹
        total_rounds_ref = [params['evaluation_rounds']]
        
        def stage2_tracker_with_round_update(current, total, question_id, process_type="", current_round=1, total_rounds=1):
            # æ›´æ–°è½®æ¬¡ä¿¡æ¯
            current_round_ref[0] = current_round
            total_rounds_ref[0] = total_rounds
            
            # è°ƒç”¨åŸå§‹çš„é—®é¢˜è¿›åº¦è·Ÿè¸ªå™¨
            stage2_tracker = create_question_progress_tracker("Stage2", file_name)
            stage2_tracker(current, total, question_id, process_type, current_round, total_rounds)
            
            # å¦‚æœæ˜¯æµ‹è¯•æˆ–è¯„ä¼°è¿‡ç¨‹ï¼Œæ›´æ–°æ€»ä½“è¿›åº¦æ¡ä»¥æ˜¾ç¤ºå½“å‰è½®æ¬¡
            if process_type in ["testing", "evaluating"]:
                current_progress = progress_base + progress_step * 0.5  # ä¼°ç®—å½“å‰è¿›åº¦
                update_overall_progress(
                    current_progress, 
                    "Stage2 æ·±åº¦æ¨ç†è¯„ä¼°", 
                    file_index=file_index,
                    file_name=file_name,
                    process_info=f"{process_type}è¿‡ç¨‹",
                    current_round=current_round,
                    total_rounds=total_rounds
                )
        
        if Path(stage1_to_stage2_path).exists():
            log_message(f"å¼€å§‹Stage2æ·±åº¦è¯„ä¼°: {file_name} ({need_retest_count}ä¸ªé—®é¢˜)")
            log_message("  Stage2å°†é‡æ–°æµ‹è¯•LLMï¼ˆä½¿ç”¨é—®é¢˜+å†…å®¹ä¸Šä¸‹æ–‡ï¼‰å¹¶é‡æ–°è¯„ä¼°")
            stage2_tracker_with_round_update(0, need_retest_count, "", "æ•°æ®åŠ è½½ä¸­")
            
            try:
                # è¯»å–éœ€è¦é‡æµ‹çš„æ•°æ®
                retest_df = pd.read_csv(stage1_to_stage2_path)
                actual_retest_count = len(retest_df)
                
                log_message(f"Stage2å®é™…å¤„ç† {actual_retest_count} ä¸ªé—®é¢˜")
                log_message("  å¼€å§‹é‡æ–°æµ‹è¯•è¿‡ç¨‹ï¼ˆæä¾›æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰")
                stage2_tracker_with_round_update(1, actual_retest_count, "", f"åŠ è½½å®Œæˆ ({actual_retest_count}ä¸ªé—®é¢˜)")
                
                # æ›´æ–°æ€»ä½“è¿›åº¦ - å¼€å§‹Stage2é‡æ–°æµ‹è¯•è¿‡ç¨‹
                update_overall_progress(
                    progress_base + progress_step * 0.1, 
                    "Stage2 æ·±åº¦æ¨ç†è¯„ä¼°", 
                    file_index=file_index,
                    file_name=file_name,
                    process_info="ğŸ¤– é‡æ–°æµ‹è¯•è¿‡ç¨‹"
                )
                
                # è®¾ç½®evaluatorçš„è¿›åº¦å›è°ƒ - è¿™é‡Œä¼šæ¥æ”¶åˆ°testingå’Œevaluatingçš„å›è°ƒ
                stage2_evaluator.set_progress_callback(stage2_tracker_with_round_update)
                
                # æ‰§è¡Œå®Œæ•´è¯„ä¼° - å†…éƒ¨ä¼šè°ƒç”¨é‡æ–°æµ‹è¯•è¿‡ç¨‹å’Œé‡æ–°è¯„ä¼°è¿‡ç¨‹
                result = stage2_evaluator.run_complete_evaluation(
                    file_paths=[stage1_to_stage2_path],
                    num_evaluations=params['evaluation_rounds'],
                    answer_threshold=params['stage2_answer_threshold'],
                    reasoning_threshold=params['stage2_reasoning_threshold']
                )
                
                # ä»ç»“æœä¸­è·å–å¤„ç†çš„æ•°æ®é‡ä¿¡æ¯
                stats = result.get('statistics', {})
                total_questions = stats.get('total_questions', actual_retest_count)
                knowledge_deficiency = stats.get('knowledge_deficiency', 0)
                reasoning_errors = stats.get('reasoning_errors', 0)
                capability_insufficient = stats.get('capability_insufficient', 0)
                
                log_message(f"Stage2å®Œæˆ - é‡æ–°æµ‹è¯•å¹¶è¯„ä¼°äº† {total_questions} ä¸ªé—®é¢˜")
                log_message(f"  - çŸ¥è¯†ç¼ºå¤±: {knowledge_deficiency}, æ¨ç†é”™è¯¯: {reasoning_errors}, èƒ½åŠ›ä¸è¶³: {capability_insufficient}")
                stage2_tracker_with_round_update(total_questions, total_questions, "", "âœ“ Stage2å®Œæˆ")
                
                # æ›´æ–°æ€»ä½“è¿›åº¦ - Stage2å®Œæˆ
                update_overall_progress(
                    progress_base + progress_step * 0.9, 
                    "Stage2 æ·±åº¦æ¨ç†è¯„ä¼°", 
                    file_index=file_index,
                    file_name=file_name,
                    process_info="é‡æ–°æµ‹è¯•+è¯„ä¼°å®Œæˆ"
                )
                
            except Exception as e:
                log_message(f"Stage2å¤„ç†å¤±è´¥: {str(e)}", "ERROR")
                stage2_tracker_with_round_update(1, 1, "", "âŒ å¤„ç†å¤±è´¥")
                
        else:
            log_message(f"æ–‡ä»¶ {file_name} çš„Stage2æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ç¬¬äºŒé˜¶æ®µ")
            stage2_tracker_with_round_update(1, 1, "", "æ•°æ®ç¼ºå¤±ï¼Œè·³è¿‡")
    
